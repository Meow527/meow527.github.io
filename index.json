[{"content":"1.nacos部署 2.redis面试题 3.RocketMQ 4.如何访问Redis中的海量数据 5.尚硅谷大数据技术之Kafka 6.虚拟机安装lnmp(工业大脑安装向导) 7.面试题MySQL篇 8.面试题高并发与大数据 9.面试题高并发和大流量解决方案篇 10.Rabbitmq ","permalink":"http://localhost:1313/2021/%E4%B8%AA%E4%BA%BA%E6%95%B4%E7%90%86pdf/","summary":"1.nacos部署 2.redis面试题 3.RocketMQ 4.如何访问Redis中的海量数据 5.尚硅谷大数据技术之Kafka 6.虚拟机安装lnmp(工业大脑安装向导) 7.面试题MySQL篇 8.面试题高并发与大数据 9.面试题高并发和大流量解决方案篇 10.Rabbitmq ","title":"个人整理pdf"},{"content":"在虚拟虚拟机中搭建redis单机集群 1.创建搭载redis到文件夹 mkdir /software/server/redis/conf 2.简单记录一下单机版的redis的下载与安装(下载到指定文件夹中) #进入文件夹 cd /software #下载包 wget https://download.redis.io/releases/redis-6.0.8.tar.gz #解压文件 tar -zxvf redis-6.0.8.tar.gz cd redis-6.0.8 #升级gcc yum install centos-release-scl yum install devtoolset-7-gcc* scl enable devtoolset-7 bash #安装 make make PREFIX=/software/server/redis install #安装到指定文件夹 #复制redis配置文件 cp /software/redis-6.0.8/redis.conf /software/server/redis/conf #进入/software/server/redis 修改配置文件 cd /software/server/redis vim conf/redis.conf #将daemonize no 改为 yes就可以了 #启动redis（进入安装redis到bin目录执行 即/software/server/redis/bin下） cd /software/server/redis/bin ./redis-server /software/server/redis/conf/redis.conf #测试连接 ./redis-cli -p 6379 #关闭redis 第一个方法 （在进入./redis-cli -p 6379之后） shutdown #关闭redis 第二个方法 #查看端口pid ps -ef ｜ grep -i redis #杀掉pid kill -9 pid 3.顺带一提其他解压命令 #其他解压命令 tar –xvf file.tar #解压 tar包 tar -zxvf file.tar.gz #解压tar.gz tar -jxvf file.tar.bz2 #解压 tar.bz2 tar –Zxvf file.tar.Z #解压tar.Z 4.redis集群搭建 #创建集群目录 mkdir redis-cluster #将redis的bin目录复制六分 cp -r /software/server/redis/bin/ /redis-cluster/redis-1 cp -r /software/server/redis/bin/ /redis-cluster/redis-2 cp -r /software/server/redis/bin/ /redis-cluster/redis-3 cp -r /software/server/redis/bin/ /redis-cluster/redis-4 cp -r /software/server/redis/bin/ /redis-cluster/redis-5 cp -r /software/server/redis/bin/ /redis-cluster/redis-6 #修改redis.conf文件 port 7001 #7001-7006以此类推 cluster-enabled yes #分别启动redis-1至redis-6(将redis.conf移动到了redis的bin目录下) cd redis-1/bin ./redis-server redis.conf cd .. \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; cd redis-2/bin \u0026amp;\u0026amp; ./redis-server redis.conf cd .. \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; cd redis-3/bin \u0026amp;\u0026amp; ./redis-server redis.conf cd .. \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; cd redis-4/bin \u0026amp;\u0026amp; ./redis-server redis.conf cd .. \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; cd redis-5/bin \u0026amp;\u0026amp; ./redis-server redis.conf cd .. \u0026amp;\u0026amp; cd .. \u0026amp;\u0026amp; cd redis-6/bin \u0026amp;\u0026amp; ./redis-server redis.conf #查看redis启动状态 ps aux | grep redis #搭建集群(redis5.0之后不需要依赖ruby) cd redis-1/bin redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 --cluster-replicas 1 #进入集群 cd /redis-cluster redis-1/bin/redis-cli -p 7001 -c 集群搭建成功如图所属 集群测试（查看集群角色 可以看到主从关系以及主从信息） info replication 常见命令 #1.获取所有的值 keys * #2.查看存储的数据剩余过期时间 ttl key #3.查看key的类型（string，set，list，hash） type key #4.查看key的值（string类型） get key #5.查看key的值（set类型） scard key #获取 set 集合中元素的数量 smembers key #查看 set 中的内容 #6.查看key的值（hash类型） hlen key #获取 key 键的字段数量 hgetall key #返回 key 键的所有字段及其值 hkeys key #获取 key 键中所有字段的名字 hvals key #获取 key 键中所有字段的值 ","permalink":"http://localhost:1313/2021/%E6%90%AD%E5%BB%BA%E5%8D%95%E6%9C%BA%E7%89%88redis%E9%9B%86%E7%BE%A4/","summary":"在虚拟虚拟机中搭建redis单机集群 1.创建搭载redis到文件夹 mkdir /software/server/redis/conf 2.简单记录一下单机版的redis的下载与安装(下载到指定文件夹中) #进入文件夹 cd /software #下载包 wget https://download.redis.io/releases/redis-6.0.8.tar.gz #解压文件 tar -zxvf redis-6.0.8.tar.gz cd redis-6.0.8 #升级gcc yum install centos-release-scl yum install devtoolset-7-gcc* scl enable devtoolset-7 bash #安装 make make PREFIX=/software/server/redis install #安装到指定文件夹 #复制redis配置文件 cp /software/redis-6.0.8/redis.conf /software/server/redis/conf #进入/software/server/redis 修改配置文件 cd /software/server/redis vim conf/redis.conf #将daemonize no 改为 yes就可以了 #启动redis（进入安装redis到bin目录执行 即/software/server/redis/bin下） cd /software/server/redis/bin ./redis-server /software/server/redis/conf/redis.conf #测试连接 ./redis-cli -p 6379 #关闭redis 第一个方法 （在进入./redis-cli -p 6379之后） shutdown #关闭redis 第二个方法 #查看端口pid ps -ef ｜ grep -i redis #杀掉pid kill -9 pid 3.","title":"搭建单机版redis集群"},{"content":"1.安装java的JDK 查看是否存在java环境 rpm -qa | grep java rpm -qa | grep jdk rpm -qa | grep gcj 如果安装过想要卸载java重新安装 rpm -qa | grep java ｜ xargs rpm -e --nodeps 查看JDK yum list java-1.8* 安装JDK yum install java-1.8.0-openjdk* -y 检查java是否安装成功 java -version 2.安装ElasticSearch 下载包 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-x86_64.rpm 安装 sudo rpm --install elasticsearch-7.12.0-x86_64.rpm 查看安装位置(通常安装在/usr/share/elasticsearch/) rpm -ql elasticsearch 修改ES数据和日志储存路径 # 创建目录，用于存储elasticsearch数据 mkdir -p /data/elasticsearch/datadir # 修改目录权限 chown -R elasticsearch:elasticsearch /data/elasticsearch/datadir # 创建日志目录 mkdir -p /data/elasticsearch/logdir # 修改目录权限 chown -R elasticsearch:elasticsearch /data/elasticsearch/logdir 修改ES的配置文件 vi /etc/elasticsearch/elasticsearch.yml #修改信息如下 # 集群名称，按照自己的需求调整cluster.name: my-application# 节点名称node.name: node-1# 设置data存储目录 path.data: /data/elasticsearch/datadir # 设置logs日志的目录 path.logs: /data/elasticsearch/logdir # 设置内存不使用交换分区 bootstrap.memory_lock: false # 设置允许所有ip可以连接该elasticsearch，这里根据项目需求自行修改 network.host: 0.0.0.0 # 开启监听的端口，默认为9200 http.port: 9527 discovery.seed_hosts:[\u0026#34;127.0.0.1\u0026#34;] cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;] #设置elasticsearch的账号密码 xpack.security.enabled: true xpack.license.self_generated.type: basic xpack.security.transport.ssl.enabled: true 进入/usr/share/elasticsearch/bin目录下执行(此处会设置六个账号 并且输入想要设置的密码) ./elasticsearch-setup-passwords interactive 如需修改密码执行 curl -H \u0026#34;Content-Type:application/json\u0026#34; -XPOST -u elastic \u0026#39;http://ip:9527/_xpack/security/user/elastic/_password\u0026#39; -d \u0026#39;{ \u0026#34;password\u0026#34; : \u0026#34;123456\u0026#34; }\u0026#39; 启动 # 启动服务 systemctl start elasticsearch # 查看运行状态 systemctl status elasticsearch # 设置开机启动 systemctl enable elasticsearch 启动失败 （如果是通过SSH远程登录的服务器，启动ES服务时会报错，原因是部署ES时创建的elasticsearch用户默认是禁止shell登录，通过切换用户命令可以看到提示：this account is currently not avaliable。 解决方案：将elasticsearch用户的shell从“/sbin /nologin”修改为“/bin/bash”即可。） vi /etc/passwd # 修改shell配置，将elasticsearch用户的shell从“/sbin/nologin”修改为“/bin/bash” elasticsearch:x:997:993:elasticsearch user:/nonexistent:/sbin/nologin =\u0026gt; elasticsearch:x:997:993:elasticsearch user:/nonexistent:/bin/bash 查看运行状态在浏览器中输入ip+端口即可 http://ip:9527 3.安装Logstash 下载包 wget https://artifacts.elastic.co/downloads/logstash/logstash-7.12.0-x86_64.rpm 安装 sudo rpm --install logstash-7.12.0-x86_64.rpm 查看安装位置(通常安装在/usr/share/logstash/) rpm -ql logstash 修改logstash数据和日志储存路径 # 创建目录，用于存储logstash数据 mkdir -p /data/logstash/datadir # 修改目录权限 chown -R logstash:logstash /data/logstash/datadir # 创建日志目录 mkdir -p /data/logstash/logdir # 修改目录权限 chown -R logstash:logstash /data/logstash/logdir 修改logstash配置文件 vi /etc/logstash/logstash.yml # 设置data存储目录 path.data: /data/logstash/datadir # 设置配置文件目录 path.config: /etc/logstash/conf.d # 设置日志存储目录 path.logs: /data/logstash/logdir 启动 # 启动服务 systemctl start logstash # 查看运行状态 systemctl status logstash # 设置开机启动 systemctl enable logstash 测试 #添加logstash的全局变量 vi /etc/profile #在里面添加 export PATH=/usr/share/elasticsearch/bin:$PATH export PATH=/usr/share/logstash/bin:$PATH #刷新环境变量 source /etc/profile #执行logstash的命令 有输出即成功 logstash -e \u0026#39;input { stdin { } } output { stdout {} }\u0026#39; 4.安装Kibana 下载包 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.12.0-x86_64.rpm 安装 sudo rpm --install kibana-7.12.0-x86_64.rpm 查看安装位置(通常安装在/usr/share/kibana/) rpm -ql kibana 修改配置信息 vi /etc/kibana/kibana.yml # 端口，默认5601 server.port: 5527 # 允许所有ip访问 server.host: \u0026#34;0.0.0.0\u0026#34; # 配置elasticsearch地址 elasticsearch.url: [\u0026#34;http://localhost:9527\u0026#34;] kibana.index: \u0026#34;.kibana\u0026#34; # elasticsearch的账号密码 elasticsearch.username: \u0026#34;elastic\u0026#34; elasticsearch.username: \u0026#34;密码\u0026#34; # 界面语言 i18n.locale:\u0026#34;zh-CN\u0026#34; 启动 # 启动服务 systemctl start kibana # 查看运行状态 systemctl status kibana # 设置开机启动 systemctl enable kibana 测试(在网页中输入ip加端口即可 然后输入账号密码即可查看kibana) http://ip:5527 整体使用参考https://www.cnblogs.com/jiangcong/p/15185016.html ","permalink":"http://localhost:1313/2021/java%E6%90%AD%E5%BB%BAelk/","summary":"1.安装java的JDK 查看是否存在java环境 rpm -qa | grep java rpm -qa | grep jdk rpm -qa | grep gcj 如果安装过想要卸载java重新安装 rpm -qa | grep java ｜ xargs rpm -e --nodeps 查看JDK yum list java-1.8* 安装JDK yum install java-1.8.0-openjdk* -y 检查java是否安装成功 java -version 2.安装ElasticSearch 下载包 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-x86_64.rpm 安装 sudo rpm --install elasticsearch-7.12.0-x86_64.rpm 查看安装位置(通常安装在/usr/share/elasticsearch/) rpm -ql elasticsearch 修改ES数据和日志储存路径 # 创建目录，用于存储elasticsearch数据 mkdir -p /data/elasticsearch/datadir # 修改目录权限 chown -R elasticsearch:elasticsearch /data/elasticsearch/datadir # 创建日志目录 mkdir -p /data/elasticsearch/logdir # 修改目录权限 chown -R elasticsearch:elasticsearch /data/elasticsearch/logdir 修改ES的配置文件 vi /etc/elasticsearch/elasticsearch.","title":"java搭建ELK"},{"content":"MQ最终一致性事务 消息百分百投递成功 （结合confirm） 消息百分百消费成功 （结合ack机制 并且要解决降幂等性问题） 关于rabbitMQ的分布式事务的大体流程 MQ分布式事务基础 1 confirm机制 (1)什么是Confirm机制 概念： Pro发送消息到Broker，Broker接受到消息后，产生回送响应Pro中有一个Confirm Listener异步监听响应。 步骤： 消息的确认Pro投递消息后，如果Broker收到消息，则会给Pro一个应答。 Pro接收应答 用来确认这条消息是否正常地发送到Broker，这也是消息可靠性投递的核心保障。 (2)Confirm机制流程图 ","permalink":"http://localhost:1313/2021/rabbitmq/","summary":"MQ最终一致性事务 消息百分百投递成功 （结合confirm） 消息百分百消费成功 （结合ack机制 并且要解决降幂等性问题） 关于rabbitMQ的分布式事务的大体流程 MQ分布式事务基础 1 confirm机制 (1)什么是Confirm机制 概念： Pro发送消息到Broker，Broker接受到消息后，产生回送响应Pro中有一个Confirm Listener异步监听响应。 步骤： 消息的确认Pro投递消息后，如果Broker收到消息，则会给Pro一个应答。 Pro接收应答 用来确认这条消息是否正常地发送到Broker，这也是消息可靠性投递的核心保障。 (2)Confirm机制流程图 ","title":"rabbitMQ"},{"content":" 1.编写一个SQL查询，获取Employee表中第二高的薪水Salary +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 例如上述Employee表，SQL查询应该返回200作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回null\n答案 --去重 select IFNULL((select Distinct Salary from Employee order by Salary DESC limit 1,1),null) as SecondHighestSalary; --小于最大值 select max(Salary) as SecondHighestSalary from Employee where Salary\u0026lt;(select max(Salary) from Employee); 2.编写一个SQL查询，获取Employee表中第n高的薪水Salary。 答案 CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN SET N= N-1; RETURN ( select IFNULL((select Distinct Salary from Employee order by Salary DESC limit N,1),null) as SecondHighestSalary ); END 3.编写一个SQL查询来实现分数排名 答案 --四个排序函数 --根据Score字段的降序排列 select Score,DENSE_RANK() over(order by Score DESC) as \u0026#39;Rank\u0026#39; from Scores; 4.编写一个SQL查询，查找所有至少连续出现三次的数字 Logs 表： +----+-----+ | Id | Num | +----+-----+ | 1 | 1 | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 1 | | 6 | 2 | | 7 | 2 | +----+-----+ Result 表：\n+-----------------+ | ConsecutiveNums | +-----------------+ | 1 | +-----------------+ 1 是唯一连续出现至少三次的数字。\n答案 select distinct Num as ConsecutiveNums from( select Num, case when @prev = Num then @count := @count + 1 when (@prev := Num) is not null then @count :=1 end as CNT from logs,(select @prev := null,@count := null) as t ) as temp where temp.CNT \u0026gt;=3 5.跟据a.DepartmentId = b.Id查出各个部门的最高薪资（表结构第六题） 答案 SELECT b.NAME AS Department, a.NAME AS Employee, a.Salary FROM Employee a LEFT JOIN Department b ON a.DepartmentId = b.Id WHERE ( a.Salary, a.DepartmentId ) IN ( SELECT max( Salary ), DepartmentId FROM Employee GROUP BY DepartmentId ); 6.找出各个部门前三位的工资 Employee 表包含所有员工信息，每个员工有其对应的工号 Id，姓名 Name，工资 Salary 和部门编号 DepartmentId 。 +----+-------+--------+--------------+ | Id | Name | Salary | DepartmentId | +----+-------+--------+--------------+ | 1 | Joe | 85000 | 1 | | 2 | Henry | 80000 | 2 | | 3 | Sam | 60000 | 2 | | 4 | Max | 90000 | 1 | | 5 | Janet | 69000 | 1 | | 6 | Randy | 85000 | 1 | | 7 | Will | 70000 | 1 | +----+-------+--------+--------------+ Department 表包含公司所有部门的信息。\n+----+----------+ | Id | Name | +----+----------+ | 1 | IT | | 2 | Sales | +----+----------+ 编写一个 SQL 查询，找出每个部门获得前三高工资的所有员工。例如，根据上述给定的表，查询结果应返回：\n+------------+----------+--------+ | Department | Employee | Salary | +------------+----------+--------+ | IT | Max | 90000 | | IT | Randy | 85000 | | IT | Joe | 85000 | | IT | Will | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | +------------+----------+--------+ 答案 #使用函数 select Department, Employee, Salary from ( select d.Name as Department, e.Name as Employee, e.Salary as Salary, #partition by是分析函数 能够在保留全部数据的基础上，只对其中某些字段做分组排序 #dense_rank() 默认从大到小排序 dense_rank() over ( partition by DepartmentId order by Salary desc) as rk from Employee as e, Department as d where e.DepartmentId = d.Id ) m where rk \u0026lt;= 3; #不使用函数 select d.Name as Department,e.Name as Employee,e.Salary as Salary from Employee as e left join Department as d on e.DepartmentId = d.Id where e.Id in ( #先把符合条件的id查询出来 select e1.Id from Employee as e1 left join Employee as e2 on e1.DepartmentId = e2.DepartmentId and e1.Salary \u0026lt;= e2.Salary group by e1.Id having count(distinct e2.Salary) \u0026lt;= 3 #工资比‘我’高的两个档位 得出90000 跟 85000两个档位的id ) and e.DepartmentId in (select Id from Department) order by d.Id asc,e.Salary desc ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql%E7%9A%84%E4%B8%80%E4%BA%9B%E9%A2%98%E7%9B%AE/","summary":"1.编写一个SQL查询，获取Employee表中第二高的薪水Salary +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 例如上述Employee表，SQL查询应该返回200作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回null\n答案 --去重 select IFNULL((select Distinct Salary from Employee order by Salary DESC limit 1,1),null) as SecondHighestSalary; --小于最大值 select max(Salary) as SecondHighestSalary from Employee where Salary\u0026lt;(select max(Salary) from Employee); 2.编写一个SQL查询，获取Employee表中第n高的薪水Salary。 答案 CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN SET N= N-1; RETURN ( select IFNULL((select Distinct Salary from Employee order by Salary DESC limit N,1),null) as SecondHighestSalary ); END 3.","title":"关于mysql的一些题目"},{"content":"Mysql应用层面的优化 1. 应用优化 在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。\n1.1 使用连接池 对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立 数据库连接池，以提高访问的性能。\n1.2 减少对MySQL的访问 1.2.1 避免对数据进行重复检索 在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。\n比如 ，需要获取书籍的id 和name字段 ， 则查询如下：\nselect id , name from tb_book; 之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下：\nselect id , status from tb_book; 这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。\nselect id, name , status from tb_book; 1.2.2 增加cache层 在应用中，我们可以在应用中增加 缓存 层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。\n因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。\n1.3 负载均衡 负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。\n1.3.1 利用MySQL复制分流查询 通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。\n1.3.2 采用分布式数据库架构 分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。\n2. Mysql中查询缓存优化 2.1 概述 开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。\n2.2 操作流程 客户端发送一条查询给服务器； 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 2.3 查询缓存配置 查看当前的MySQL数据库是否支持查询缓存：\nSHOW VARIABLES LIKE \u0026#39;have_query_cache\u0026#39;;\t查看当前MySQL是否开启了查询缓存 ：\nSHOW VARIABLES LIKE \u0026#39;query_cache_type\u0026#39;; 查看查询缓存的占用大小 ：\nSHOW VARIABLES LIKE \u0026#39;query_cache_size\u0026#39;; 查看查询缓存的状态变量：\nSHOW STATUS LIKE \u0026#39;Qcache%\u0026#39;; 各个变量的含义如下：\n参数 含义 Qcache_free_blocks 查询缓存中的可用内存块数 Qcache_free_memory 查询缓存的可用内存量 Qcache_hits 查询缓存命中数 Qcache_inserts 添加到查询缓存的查询数 Qcache_lowmen_prunes 由于内存不足而从查询缓存中删除的查询数 Qcache_not_cached 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） Qcache_queries_in_cache 查询缓存中注册的查询数 Qcache_total_blocks 查询缓存中的块总数 2.4 开启查询缓存 MySQL的查询缓存默认是关闭的，需要手动配置参数 query_cache_type ， 来开启查询缓存。query_cache_type 该参数的可取值有三个 ：\n值 含义 OFF 或 0 查询缓存功能关闭 ON 或 1 查询缓存功能打开，SELECT的结果符合缓存条件即会缓存，否则，不予缓存，显式指定 SQL_NO_CACHE，不予缓存 DEMAND 或 2 查询缓存功能按需进行，显式指定 SQL_CACHE 的SELECT语句才会缓存；其它均不予缓存 在 /usr/my.cnf 配置中，增加以下配置 ：\n配置完毕之后，重启服务既可生效 ；\n然后就可以在命令行执行SQL语句进行验证 ，执行一条比较耗时的SQL语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。\n2.5 查询缓存SELECT选项 可以在SELECT语句中指定两个与查询缓存相关的选项 ：\nSQL_CACHE : 如果查询结果是可缓存的，并且 query_cache_type 系统变量的值为ON或 DEMAND ，则缓存查询结果 。\nSQL_NO_CACHE : 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。\n例子：\nSELECT SQL_CACHE id, name FROM customer; SELECT SQL_NO_CACHE id, name FROM customer; ​\n2.6 查询缓存失效的情况 1） SQL 语句不一致的情况， 要想命中查询缓存，查询的SQL语句必须一致。\nSQL1 : select count(*) from tb_item; SQL2 : Select count(*) from tb_item; 2） 当查询语句中有一些不确定的时，则不会缓存。如 ： now() , current_date() , curdate() , curtime() , rand() , uuid() , user() , database() 。\nSQL1 : select * from tb_item where updatetime \u0026lt; now() limit 1; SQL2 : select user(); SQL3 : select database(); 3） 不使用任何表查询语句。\nselect \u0026#39;A\u0026#39;; 4） 查询 mysql， information_schema或 performance_schema 数据库中的表时，不会走查询缓存。\nselect * from information_schema.engines; 5） 在存储的函数，触发器或事件的主体内执行的查询。\n6） 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用MERGE映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变 INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE 。\n3. Mysql内存管理及优化 3.1 内存优化原则 1） 将尽量多的内存分配给MySQL做缓存，但要给操作系统和其他程序预留足够内存。\n2） MyISAM 存储引擎的数据文件读取依赖于操作系统自身的IO缓存，因此，如果有MyISAM表，就要预留更多的内存给操作系统做IO缓存。\n3） 排序区、连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发连接较高时会导致物理内存耗尽。\n3.2 MyISAM 内存优化 myisam存储引擎使用 key_buffer 缓存索引块，加速myisam索引的读写速度。对于myisam表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。\nkey_buffer_size key_buffer_size决定MyISAM索引块缓存区的大小，直接影响到MyISAM表的存取效率。可以在MySQL参数文件中设置key_buffer_size的值，对于一般MyISAM数据库，建议至少将1/4可用内存分配给key_buffer_size。\n在/usr/my.cnf 中做如下配置：\nkey_buffer_size=512M read_buffer_size 如果需要经常顺序扫描myisam表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个session独占的，如果默认值设置太大，就会造成内存浪费。\nread_rnd_buffer_size 对于需要做排序的myisam表的查询，如带有order by子句的sql，适当增加 read_rnd_buffer_size 的值，可以改善此类的sql性能。但需要注意的是 read_rnd_buffer_size 是每个session独占的，如果默认值设置太大，就会造成内存浪费。\n3.3 InnoDB 内存优化 innodb用一块内存区做IO缓存池，该缓存池不仅用来缓存innodb的索引块，而且也用来缓存innodb的数据块。\ninnodb_buffer_pool_size 该变量决定了 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问InnoDB表需要的磁盘I/O 就越少，性能也就越高。\ninnodb_buffer_pool_size=512M innodb_log_buffer_size 决定了innodb重做日志缓存的大小，对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免innodb在事务提交前就执行不必要的日志写入磁盘操作。\ninnodb_log_buffer_size=10M 4. Mysql并发参数调整 从实现上来说，MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在Mysql中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size、table_open_cahce。\n4.1 max_connections 采用max_connections 控制允许连接到MySQL数据库的最大数量，默认值是 151。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。\nMysql 最大可支持的连接数，取决于很多因素，包括给定操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度，期望的响应时间等。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。\n4.2 back_log back_log 参数控制MySQL监听TCP端口时设置的积压请求栈大小。如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 但最大不超过900。\n如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。\n4.3 table_open_cache 该参数用来控制所有SQL语句执行线程可打开表缓存的数量， 而在执行SQL语句时，每一个SQL执行线程至少要打开 1 个表缓存。该参数的值应该根据设置的最大连接数 max_connections 以及每个连接执行关联查询中涉及的表的最大数量来设定 ：\n​\tmax_connections x N ；\n4.4 thread_cache_size 为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户服务线程的数量。\n4.5 innodb_lock_wait_timeout 该参数是用来设置InnoDB 事务等待行锁的时间，默认值是50ms ， 可以根据需要进行动态设置。对于需要快速反馈的业务系统来说，可以将行锁的等待时间调小，以避免事务长时间挂起； 对于后台运行的批量处理程序来说， 可以将行锁的等待时间调大， 以避免发生大的回滚操作。\n5. Mysql锁问题 5.1 锁概述 锁是计算机协调多个进程或线程并发访问某一资源的机制（避免争抢）。\n在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。\n5.2 锁分类 从对数据操作的粒度分 ：\n1） 表锁：操作时，会锁定整个表。\n2） 行锁：操作时，会锁定当前操作行。\n从对数据操作的类型分：\n1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。\n2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。\n5.3 Mysql 锁 相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况：\n存储引擎 表级锁 行级锁 页面锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 MySQL这3种锁的特性可大致归纳如下 ：\n锁类型 特点 表级锁 偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁 偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理（OLTP）系统。\n5.2 MyISAM 表锁 MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。\n5.2.1 如何加表锁 MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。\n显示加表锁语法：\n加读锁 ： lock table table_name read; 加写锁 ： lock table table_name write； 5.2.2 读锁案例 准备环境\ncreate database demo_03 default charset=utf8mb4; use demo_03; CREATE TABLE `tb_book` ( `id` INT(11) auto_increment, `name` VARCHAR(50) DEFAULT NULL, `publish_time` DATE DEFAULT NULL, `status` CHAR(1) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=myisam DEFAULT CHARSET=utf8 ; INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,\u0026#39;java编程思想\u0026#39;,\u0026#39;2088-08-01\u0026#39;,\u0026#39;1\u0026#39;); INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,\u0026#39;solr编程思想\u0026#39;,\u0026#39;2088-08-08\u0026#39;,\u0026#39;0\u0026#39;); CREATE TABLE `tb_user` ( `id` INT(11) auto_increment, `name` VARCHAR(50) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=myisam DEFAULT CHARSET=utf8 ; INSERT INTO tb_user (id, name) VALUES(NULL,\u0026#39;令狐冲\u0026#39;); INSERT INTO tb_user (id, name) VALUES(NULL,\u0026#39;田伯光\u0026#39;); 客户端 一 ：\n1）获得tb_book 表的读锁\nlock table tb_book read; 2） 执行查询操作\nselect * from tb_book; 可以正常执行 ， 查询出数据。\n客户端 二 ：\n3） 执行查询操作\nselect * from tb_book; 客户端 一 ：\n4）查询未锁定的表\nselect name from tb_seller; 客户端 二 ：\n5）查询未锁定的表\nselect name from tb_seller; 可以正常查询出未锁定的表；\n客户端 一 ：\n6） 执行插入操作\ninsert into tb_book values(null,\u0026#39;Mysql高级\u0026#39;,\u0026#39;2088-01-01\u0026#39;,\u0026#39;1\u0026#39;); 执行插入， 直接报错 ， 由于当前tb_book 获得的是 读锁， 不能执行更新操作。\n客户端 二 ：\n7） 执行插入操作\ninsert into tb_book values(null,\u0026#39;Mysql高级\u0026#39;,\u0026#39;2088-01-01\u0026#39;,\u0026#39;1\u0026#39;); 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 inesrt 语句 ， 立即执行 ；\n5.2.3 写锁案例 客户端 一 :\n1）获得tb_book 表的写锁\nlock table tb_book write ; 2）执行查询操作\nselect * from tb_book ; 查询操作执行成功；\n3）执行更新操作\nupdate tb_book set name = \u0026#39;java编程思想（第二版）\u0026#39; where id = 1; 更新操作执行成功 ；\n客户端 二 :\n4）执行查询操作\nselect * from tb_book ; 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 select 语句 ， 立即执行 ；\n5.2.4 结论 锁模式的相互兼容性如表中所示：\n由上表可见：\n​\t1） 对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；\n​\t2） 对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作；\n​\t简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。\n此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。\n5.2.5 查看锁的争用情况 show open tables； In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。\nName_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。\nshow status like \u0026#39;Table_locks%\u0026#39;; Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。\nTable_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。\n5.3 InnoDB 行锁 5.3.1 行锁介绍 行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。\nInnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是 采用了行级锁。\n5.3.2 背景知识 事务及其ACID属性\n事务是由一组SQL语句组成的逻辑处理单元。\n事务具有以下4个特性，简称为事务ACID属性。\nACID属性 含义 原子性（Atomicity） 事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。 一致性（Consistent） 在事务开始和完成时，数据都必须保持一致状态。 隔离性（Isolation） 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。 持久性（Durable） 事务完成之后，对于数据的修改是永久的。 并发事务处理带来的问题\n问题 含义 丢失更新（Lost Update） 当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。 脏读（Dirty Reads） 当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。 幻读（Phantom Reads） 一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。 事务隔离级别\n为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。\n数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。\n隔离级别 丢失更新 脏读 不可重复读 幻读 Read uncommitted × √ √ √ Read committed × × √ √ Repeatable read（默认） × × × √ Serializable × × × × 备注 ： √ 代表可能出现 ， × 代表不会出现 。\nMysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式：\nshow variables like \u0026#39;tx_isolation\u0026#39;; 5.3.3 InnoDB 的行锁模式 InnoDB 实现了以下两种类型的行锁。\n共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。 排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。 对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；\n对于普通SELECT语句，InnoDB不会加任何锁；\n可以通过以下语句显示给记录集加共享锁或排他锁 。\n共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE 排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE 5.3.4 案例准备工作 create table test_innodb_lock( id int(11), name varchar(16), sex varchar(1) )engine = innodb default charset=utf8; insert into test_innodb_lock values(1,\u0026#39;100\u0026#39;,\u0026#39;1\u0026#39;); insert into test_innodb_lock values(3,\u0026#39;3\u0026#39;,\u0026#39;1\u0026#39;); insert into test_innodb_lock values(4,\u0026#39;400\u0026#39;,\u0026#39;0\u0026#39;); insert into test_innodb_lock values(5,\u0026#39;500\u0026#39;,\u0026#39;1\u0026#39;); insert into test_innodb_lock values(6,\u0026#39;600\u0026#39;,\u0026#39;0\u0026#39;); insert into test_innodb_lock values(7,\u0026#39;700\u0026#39;,\u0026#39;0\u0026#39;); insert into test_innodb_lock values(8,\u0026#39;800\u0026#39;,\u0026#39;1\u0026#39;); insert into test_innodb_lock values(9,\u0026#39;900\u0026#39;,\u0026#39;1\u0026#39;); insert into test_innodb_lock values(1,\u0026#39;200\u0026#39;,\u0026#39;0\u0026#39;); create index idx_test_innodb_lock_id on test_innodb_lock(id); create index idx_test_innodb_lock_name on test_innodb_lock(name); 5.3.5 行锁基本演示 Session-1 Session-2 关闭自动提交功能 关闭自动提交功能 可以正常的查询出全部的数据 可以正常的查询出全部的数据 查询id 为3的数据 ； 获取id为3的数据 ； 更新id为3的数据，但是不提交； 更新id为3 的数据， 出于等待状态 通过commit， 提交事务 解除阻塞，更新正常进行 以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ： 更新id为3数据，正常的获取到行锁 ， 执行更新 ； 由于与Session-1 操作不是同一行，获取当前行锁，执行更新； 5.3.6 无索引行锁升级为表锁 如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。\n查看当前表的索引 ： show index from test_innodb_lock ;\nSession-1 Session-2 关闭事务的自动提交 关闭事务的自动提交 执行更新语句 ： 执行更新语句， 但处于阻塞状态： 提交事务： 解除阻塞，执行更新成功 ： 执行提交操作 ： 由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数组类型使用，存在类型转换，索引失效，最终行锁变为表锁 ；\n5.3.7 间隙锁危害 当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 \u0026ldquo;间隙（GAP）\u0026rdquo; ， InnoDB也会对这个 \u0026ldquo;间隙\u0026rdquo; 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。\n示例 ：\nSession-1 Session-2 关闭事务自动提交 关闭事务自动提交 根据id范围更新数据 插入id为2的记录， 出于阻塞状态 提交事务 ； 解除阻塞 ， 执行插入操作 ： 提交事务 ： 5.3.8 InnoDB 行锁争用情况 show status like \u0026#39;innodb_row_lock%\u0026#39;; Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg:每次等待所花平均时长 Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间 Innodb_row_lock_waits: 系统启动后到现在总共等待的次数 当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 5.3.9 总结 InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远由于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。\n但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。\n优化建议：\n尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件，及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（但是需要业务层面满足需求） 6. 常用SQL技巧 6.1 SQL执行顺序 编写顺序\nSELECT DISTINCT \u0026lt;select list\u0026gt; FROM \u0026lt;left_table\u0026gt; \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;join_condition\u0026gt; WHERE \u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_list\u0026gt; HAVING \u0026lt;having_condition\u0026gt; ORDER BY \u0026lt;order_by_condition\u0026gt; LIMIT \u0026lt;limit_params\u0026gt; 执行顺序\nFROM\t\u0026lt;left_table\u0026gt; ON \u0026lt;join_condition\u0026gt; \u0026lt;join_type\u0026gt;\tJOIN\t\u0026lt;right_table\u0026gt; WHERE\t\u0026lt;where_condition\u0026gt; GROUP BY \u0026lt;group_by_list\u0026gt; HAVING\t\u0026lt;having_condition\u0026gt; SELECT DISTINCT\t\u0026lt;select list\u0026gt; ORDER BY\t\u0026lt;order_by_condition\u0026gt; LIMIT\t\u0026lt;limit_params\u0026gt; 6.2 正则表达式使用 正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。\n符号 含义 ^ 在字符串开始处进行匹配 $ 在字符串末尾处进行匹配 . 匹配任意单个字符, 包括换行符 [\u0026hellip;] 匹配出括号内的任意字符 [^\u0026hellip;] 匹配不出括号内的任意字符 a* 匹配零个或者多个a(包括空串) a+ 匹配一个或者多个a(不包括空串) a? 匹配零个或者一个a a1|a2 匹配a1或a2 a(m) 匹配m个a a(m,) 至少匹配m个a a(m,n) 匹配m个a 到 n个a a(,n) 匹配0到n个a (\u0026hellip;) 将模式元素组成单一元素 select * from emp where name regexp \u0026#39;^T\u0026#39;; select * from emp where name regexp \u0026#39;2$\u0026#39;; select * from emp where name regexp \u0026#39;[uvw]\u0026#39;; 6.3 MySQL 常用函数 数字函数\n函数名称 作 用 ABS 求绝对值 SQRT 求二次方根 MOD 求余数 CEIL 和 CEILING 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 FLOOR 向下取整，返回值转化为一个BIGINT RAND 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列 ROUND 对所传参数进行四舍五入 SIGN 返回参数的符号 POW 和 POWER 两个函数的功能相同，都是所传参数的次方的结果值 SIN 求正弦值 ASIN 求反正弦值，与函数 SIN 互为反函数 COS 求余弦值 ACOS 求反余弦值，与函数 COS 互为反函数 TAN 求正切值 ATAN 求反正切值，与函数 TAN 互为反函数 COT 求余切值 字符串函数\n函数名称 作 用 LENGTH 计算字符串长度函数，返回字符串的字节长度 CONCAT 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 INSERT 替换字符串函数 LOWER 将字符串中的字母转换为小写 UPPER 将字符串中的字母转换为大写 LEFT 从左侧字截取符串，返回字符串左边的若干个字符 RIGHT 从右侧字截取符串，返回字符串右边的若干个字符 TRIM 删除字符串左右两侧的空格 REPLACE 字符串替换函数，返回替换后的新字符串 SUBSTRING 截取字符串，返回从指定位置开始的指定长度的字符换 REVERSE 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串 日期函数\n函数名称 作 用 CURDATE 和 CURRENT_DATE 两个函数作用相同，返回当前系统的日期值 CURTIME 和 CURRENT_TIME 两个函数作用相同，返回当前系统的时间值 NOW 和 SYSDATE 两个函数作用相同，返回当前系统的日期和时间值 MONTH 获取指定日期中的月份 MONTHNAME 获取指定日期中的月份英文名称 DAYNAME 获取指定曰期对应的星期几的英文名称 DAYOFWEEK 获取指定日期对应的一周的索引位置值 WEEK 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 DAYOFYEAR 获取指定曰期是一年中的第几天，返回值范围是1~366 DAYOFMONTH 获取指定日期是一个月中是第几天，返回值范围是1~31 YEAR 获取年份，返回值范围是 1970〜2069 TIME_TO_SEC 将时间参数转换为秒数 SEC_TO_TIME 将秒数转换为时间，与TIME_TO_SEC 互为反函数 DATE_ADD 和 ADDDATE 两个函数功能相同，都是向日期添加指定的时间间隔 DATE_SUB 和 SUBDATE 两个函数功能相同，都是向日期减去指定的时间间隔 ADDTIME 时间加法运算，在原始时间上添加指定的时间 SUBTIME 时间减法运算，在原始时间上减去指定的时间 DATEDIFF 获取两个日期之间间隔，返回参数 1 减去参数 2 的值 DATE_FORMAT 格式化指定的日期，根据参数返回指定格式的值 WEEKDAY 获取指定日期在一周内的对应的工作日索引 聚合函数\n函数名称 作用 MAX 查询指定列的最大值 MIN 查询指定列的最小值 COUNT 统计查询结果的行数 SUM 求和，返回指定列的总和 AVG 求平均值，返回指定列数据的平均值 ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql%E5%BA%94%E7%94%A8%E5%B1%82%E9%9D%A2%E7%9A%84%E4%BC%98%E5%8C%96/","summary":"Mysql应用层面的优化 1. 应用优化 在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。\n1.1 使用连接池 对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立 数据库连接池，以提高访问的性能。\n1.2 减少对MySQL的访问 1.2.1 避免对数据进行重复检索 在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。\n比如 ，需要获取书籍的id 和name字段 ， 则查询如下：\nselect id , name from tb_book; 之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下：\nselect id , status from tb_book; 这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。\nselect id, name , status from tb_book; 1.2.2 增加cache层 在应用中，我们可以在应用中增加 缓存 层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。\n因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。\n1.3 负载均衡 负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。\n1.3.1 利用MySQL复制分流查询 通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。\n1.3.2 采用分布式数据库架构 分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。\n2. Mysql中查询缓存优化 2.1 概述 开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。\n2.2 操作流程 客户端发送一条查询给服务器； 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 2.","title":"关于mysql应用层面的优化"},{"content":"1. 优化insert语句 当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。\n如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。\n示例， 原始方式为：\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); 优化后的方案为 ：\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;),(2,\u0026#39;Cat\u0026#39;)，(3,\u0026#39;Jerry\u0026#39;); 在事务中进行数据插入。\nstart transaction; insert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); commit; 数据有序插入\ninsert into tb_test values(4,\u0026#39;Tim\u0026#39;); insert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); insert into tb_test values(5,\u0026#39;Rose\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); 优化后\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); insert into tb_test values(4,\u0026#39;Tim\u0026#39;); insert into tb_test values(5,\u0026#39;Rose\u0026#39;); 2. 优化order by语句 2.1 环境准备 CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(100) NOT NULL, `age` int(3) NOT NULL, `salary` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;1\u0026#39;,\u0026#39;Tom\u0026#39;,\u0026#39;25\u0026#39;,\u0026#39;2300\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;2\u0026#39;,\u0026#39;Jerry\u0026#39;,\u0026#39;30\u0026#39;,\u0026#39;3500\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;3\u0026#39;,\u0026#39;Luci\u0026#39;,\u0026#39;25\u0026#39;,\u0026#39;2800\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;4\u0026#39;,\u0026#39;Jay\u0026#39;,\u0026#39;36\u0026#39;,\u0026#39;3500\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;5\u0026#39;,\u0026#39;Tom2\u0026#39;,\u0026#39;21\u0026#39;,\u0026#39;2200\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;6\u0026#39;,\u0026#39;Jerry2\u0026#39;,\u0026#39;31\u0026#39;,\u0026#39;3300\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;7\u0026#39;,\u0026#39;Luci2\u0026#39;,\u0026#39;26\u0026#39;,\u0026#39;2700\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;8\u0026#39;,\u0026#39;Jay2\u0026#39;,\u0026#39;33\u0026#39;,\u0026#39;3500\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;9\u0026#39;,\u0026#39;Tom3\u0026#39;,\u0026#39;23\u0026#39;,\u0026#39;2400\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;10\u0026#39;,\u0026#39;Jerry3\u0026#39;,\u0026#39;32\u0026#39;,\u0026#39;3100\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;11\u0026#39;,\u0026#39;Luci3\u0026#39;,\u0026#39;26\u0026#39;,\u0026#39;2900\u0026#39;); insert into `emp` (`id`, `name`, `age`, `salary`) values(\u0026#39;12\u0026#39;,\u0026#39;Jay3\u0026#39;,\u0026#39;37\u0026#39;,\u0026#39;4500\u0026#39;); create index idx_emp_age_salary on emp(age,salary); 2.2 两种排序方式 1). 第一种是通过对返回数据进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。\n--FileSort效率比较低 explain select * from 表 order by 字段 DESC; 2). 第二种通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。 多字段排序\n--id为主键 解析中的extra都会为Using index explain select id from 表 order by id DESC; --添加了关于age、salary的索引 解析中的extra都会为Using index --相当于使用覆盖索引，其效率会变高 explain select id,age,salary from 表 order by id DESC; 了解了MySQL的排序方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where 条件和Order by 使用相同的索引，并且Order By 的顺序和索引顺序相同， 并且Order by 的字段都是升序，或者都是降序。否则肯定需要额外的操作，这样就会出现FileSort。\n2.3 Filesort 的优化 通过创建合适的索引，能够减少 Filesort 的出现，但是在某些情况下，条件限制不能让Filesort消失，那就需要加快 Filesort的排序操作。对于Filesort ， MySQL 有两种排序算法：\n1） 两次扫描算法 ：MySQL4.1 之前，使用该方式排序。首先根据条件取出排序字段和行指针信息，然后在排序区 sort buffer 中排序，如果sort buffer不够，则在临时表 temporary table 中存储排序结果。完成排序之后，再根据行指针回表读取记录，该操作可能会导致大量随机I/O操作。\n2）一次扫描算法：一次性取出满足条件的所有字段，然后在排序区 sort buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法要高。\nMySQL 通过比较系统变量 max_length_for_sort_data 的大小和Query语句取出的字段总大小， 来判定是否那种排序算法，如果max_length_for_sort_data 更大，那么使用第二种优化之后的算法；否则使用第一种。 可以适当提高 sort_buffer_size 和 max_length_for_sort_data 系统变量，来增大排序区的大小，提高排序的效率。\nshow variables like \u0026#39;max_length_for_sort_data\u0026#39;; show variables like \u0026#39;sort_buffer_size\u0026#39;; 3. 优化group by 语句 由于GROUP BY 实际上也同样会进行排序操作，而且与ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作。当然，如果在分组的时候还使用了其他的一些聚合函数，那么还需要一些聚合函数的计算。所以，在GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。\n如果查询包含 group by 但是用户想要避免排序结果的消耗， 则可以执行order by null 禁止排序。如下 ：\nexplain select age,count(*) from emp group by age; 优化后\nexplain select age,count(*) from emp group by age order by null; 从上面的例子可以看出，第一个SQL语句需要进行\u0026quot;filesort\u0026quot;，而第二个SQL由于order by null 不需要进行 \u0026ldquo;filesort\u0026rdquo;， 而上文提过Filesort往往非常耗费时间。\n创建索引 ：\ncreate index idx_emp_age_salary on emp(age,salary)； --在执行 explain里面的extra 就会变成Using index 查询速度会变得更快 explain select age,count(*) from emp group by age order by null; 4. 优化嵌套查询 Mysql4.1版本之后，开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询是可以被更高效的连接（JOIN）替代。\n示例 ，查找有角色的所有的用户信息 :\nexplain select * from t_user where id in (select user_id from user_role ); 优化后 :\nexplain select * from t_user u , user_role ur where u.id = ur.user_id; 连接(Join)查询之所以更有效率一些 ，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上需要两个步骤的查询工作。\n5. 优化OR条件 对于包含OR的查询子句，如果要利用索引，则OR之间的每个条件列都必须用到索引 ， 而且不能使用到复合索引； 如果没有索引，则应该考虑增加索引。\n建议使用 union 替换 or ：\nexplain select * from emp_test where id = 1 or id = 10; --优化单列 explain select * from emp_test where id = 1 union select * from emp_test where id = 10; explain select * from emp_test where id = 1 or age = 10; --优化 explain select * from emp_test where id = 1 union select * from emp_test where age = 10; 我们来比较下重要指标，发现主要差别是 type 和 ref 这两项\ntype 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是：\nsystem \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt; unique_subquery \u0026gt; index_subquery \u0026gt; range \u0026gt; index \u0026gt; ALL UNION 语句的 type 值为 ref，OR 语句的 type 值为 range，可以看到这是一个很明显的差距\nUNION 语句的 ref 值为 const，OR 语句的 type 值为 null，const 表示是常量值引用，非常快\n这两项的差距就说明了 UNION 要优于 OR 。\n6. 优化分页查询 一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10 ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。\n6.1 优化思路一 在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。\nSELECT * FROM test_user a,(SELECT id from test_user limit 4000000,10) b where a.id = b.id; SELECT * FROM test_user WHERE id \u0026gt;= (SELECT id FROM test_user ORDER BY id ASC LIMIT 6000000, 1) LIMIT 10; SELECT * FROM test_user a,(SELECT id from test_user ORDER BY id ASC limit 6000000,10) b where a.id = b.id; 6.2 优化思路二 该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。\nSELECT * FROM test_user where id \u0026gt; 2000000 limit 10; SELECT * FROM `test_user` WHERE id between 6000137 AND 6000146 LIMIT 10; SELECT * FROM `test_user` WHERE id \u0026gt;= 6000137 LIMIT 100; 7. 使用SQL提示 SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。\n7.1 USE INDEX 在查询语句中表名的后面，添加 use index 来提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。\ncreate index idx_seller_name on 表名(字段); select * from 表名 use index(idx_seller_name) where name = \u0026#39;\u0026#39;; 7.2 IGNORE INDEX 如果用户只是单纯的想让MySQL忽略一个或者多个索引，则可以使用 ignore index 作为 hint 。\nexplain select * from tb_seller ignore index(idx_seller_name) where name = \u0026#39;\u0026#39;; 7.3 FORCE INDEX 为强制MySQL使用一个特定的索引，可在查询中使用 force index 作为hint 。\ncreate index idx_seller_address on tb_seller(address); select * from 表名 FORCE index(idx_seller_name) where name = \u0026#39;\u0026#39;; ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql%E7%9A%84sql%E4%BC%98%E5%8C%96/","summary":"1. 优化insert语句 当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。\n如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。\n示例， 原始方式为：\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); 优化后的方案为 ：\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;),(2,\u0026#39;Cat\u0026#39;)，(3,\u0026#39;Jerry\u0026#39;); 在事务中进行数据插入。\nstart transaction; insert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); commit; 数据有序插入\ninsert into tb_test values(4,\u0026#39;Tim\u0026#39;); insert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); insert into tb_test values(5,\u0026#39;Rose\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); 优化后\ninsert into tb_test values(1,\u0026#39;Tom\u0026#39;); insert into tb_test values(2,\u0026#39;Cat\u0026#39;); insert into tb_test values(3,\u0026#39;Jerry\u0026#39;); insert into tb_test values(4,\u0026#39;Tim\u0026#39;); insert into tb_test values(5,\u0026#39;Rose\u0026#39;); 2.","title":"关于mysql的sql语句的优化"},{"content":"区别： 1.hash不支持最左原则，b+tree支持。因为hash的联合查询是这样，比如where a=1 and b=2，是把 a=1 and b=2转成一个hash码来进行查询，如果换成 b=2 and a=1，那么hash码将完全不同，索引失效。\n2.hash不支持范围查询，因为hash是无序的，和b+tree不一样，b树是有序的\n3.hash不支持order by，因为是无序的\n4.hash不支持模糊查询\n5.如果是等值查询，那么哈希索引明显有绝对优势，hash在等值查询上效率比btree高，因为hash可以根据查询条件直接找到需要的数据，但是在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。\n","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql-innodb-hash%E4%B8%8Eb-tree%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"区别： 1.hash不支持最左原则，b+tree支持。因为hash的联合查询是这样，比如where a=1 and b=2，是把 a=1 and b=2转成一个hash码来进行查询，如果换成 b=2 and a=1，那么hash码将完全不同，索引失效。\n2.hash不支持范围查询，因为hash是无序的，和b+tree不一样，b树是有序的\n3.hash不支持order by，因为是无序的\n4.hash不支持模糊查询\n5.如果是等值查询，那么哈希索引明显有绝对优势，hash在等值查询上效率比btree高，因为hash可以根据查询条件直接找到需要的数据，但是在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。","title":"关于mysql InnoDB hash与B+tree的区别"},{"content":"在MySQL中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 mysql的事务可以理解成用来维护数据库表的完整性。 mysql事务的四个特点 原子性 : 一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样 一致性 : 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。 隔离性 : 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性 : 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考链接 https://www.runoob.com/mysql/mysql-transaction.html ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql%E4%BA%8B%E5%8A%A1%E7%9A%84%E6%A6%82%E5%BF%B5/","summary":"在MySQL中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 mysql的事务可以理解成用来维护数据库表的完整性。 mysql事务的四个特点 原子性 : 一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样 一致性 : 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。 隔离性 : 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性 : 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考链接 https://www.runoob.com/mysql/mysql-transaction.html ","title":"关于mysql事务的概念"},{"content":"索引 可以理解成:索引就是mysql高速获取数据的一种数据结构。 聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。 MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 B+树数据存叶子节点。索引存根节点。 优势 1） 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。 2） 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。 劣势 1） 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。 2） 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 索引设计原则 索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。 对查询频次较高，且数据量比较大的表建立索引。 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。 使用唯一索引，区分度越高，使用索引的效率越高。 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。 利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。 避免索引失效 全匹配原则。 最左前缀法则。 在最左前缀法则的基础上，范围查询右边的列，不会使用索引。 不要在索引列上进行运算操作， 索引将失效。 字符串类型不加单引号查询， 索引将失效。 尽量使用覆盖索引，避免使用select *（避免回表查询 什么是回表查询:mysql的索引是一个链子 在查找数据的时候找的是链子上面的索引，在找到链子的东西，select * 通过链子的索引的东西再去数据库表把整条找出来，这就叫索引回表。 避免回表查询：在建立复合索引时尽量把）； 避免or跟索引条件连用 如果仅仅是尾部模糊匹配（like \u0026lsquo;str%\u0026rsquo;），索引不会失效。如果是头部模糊匹配，索引失效。（用覆盖索引能避免模糊查询失效） 如果MySQL评估使用索引比全表更慢，则不使用索引。 单列索引和复合索引。尽量使用复合索引，而少使用单列索引 。 存储过程和函数 存储过程和函数概述 存储过程和函数是 事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 存储过程和函数的区别在于函数必须有返回值，而存储过程没有。 函数 ： 是一个有返回值的过程 ； 过程 ： 是一个没有返回值的函数 ； 各种存储引擎特性 下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ： 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 有 有 没有 有 事务安全 ==支持== 锁机制 ==行锁(适合高并发)== ==表锁== 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.6版本之后) 支持 集群索引 支持 数据索引 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 高 低 N/A 低 低 内存使用 高 低 中等 低 高 批量插入速度 低 高 高 高 高 支持外键 ==支持== InnoDB InnoDB存储引擎是Mysql的默认存储引擎。InnoDB存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。 特点：支持事务控制和外键约束。\n存储表和索引有以下两种方式 ： ①. 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。 ②. 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。 MyISAM MyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT 为主的应用或允许少量数据丢失 基本上都可以使用这个引擎来创建表 。 特点：不支持事务和外键，所以访问速度很快。\n文件存储方式 ①.文件存储方式 每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ：.frm (存储表定义)； .MYD(MYData , 存储数据)； .MYI(MYIndex , 存储索引)； MEMORY Memory存储引擎将表的数据存放在内存中。每个MEMORY表实际对应一个磁盘文件，格式是.frm ，该文件中只存储表的结构，而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。MEMORY 类型的表访问非常地快，因为他的数据是存放在内存中的，并且默认使用HASH索引 ， 但是服务一旦关闭，表中的数据就会丢失。 MERGE MERGE存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，MERGE表本身并没有存储数据，对MERGE类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行的。 对于MERGE类型表的插入操作，是通过INSERT_METHOD子句定义插入的表，可以有3个不同的值，使用FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上，不定义这个子句或者定义为NO，表示不能对这个MERGE表执行插入操作。 可以对MERGE表进行DROP操作，但是这个操作只是删除MERGE表的定义，对内部的表是没有任何影响的。 存储引擎的选择 在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。以下是几种常用的存储引擎的使用环境。 InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。 MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。 MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。 MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。这对于存储诸如数据仓储等VLDB环境十分合适。 优化SQL 查看SQL执行频率 MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。show [session|global] status 可以根据需要加上参数“session”或者“global”来显示 session 级（当前连接）的计结果和 global 级（自数据库上次启动至今）的统计结果。如果不写，默认使用参数是“session”。 show session status like \u0026#39;com_______\u0026#39;; show global status like \u0026#39;com_______\u0026#39;; -- 查看innodb数据库影响的行数 show status like \u0026#39;Innodb_rows_%\u0026#39;; 含义\n参数 含义 Com_select 执行 select 操作的次数，一次查询只累加 1。 Com_insert 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次。 Com_update 执行 UPDATE 操作的次数。 Com_delete 执行 DELETE 操作的次数。 Innodb_rows_read select 查询返回的行数。 Innodb_rows_inserted 执行 INSERT 操作插入的行数。 Innodb_rows_updated 执行 UPDATE 操作更新的行数。 Innodb_rows_deleted 执行 DELETE 操作删除的行数。 Connections 试图连接 MySQL 服务器的次数。 Uptime 服务器工作时间。 Slow_queries 慢查询的次数。 Com_*** : 这些参数对于所有存储引擎的表操作都会进行累计。\nInnodb_*** : 这几个参数只是针对InnoDB 存储引擎的，累加的算法也略有不同。\n定位低效率执行SQL 可以通过以下两种方式定位执行效率较低的 SQL 语句。 慢查询日志 : 通过慢查询日志定位那些执行效率较低的 SQL 语句，用\u0026ndash;log-slow-queries[=file_name]选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件。具体可以查看本书第 26 章中日志管理的相关部分。 show processlist : 慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题，可以使用show processlist命令查看当前MySQL在进行的线程，包括线程的状态、是否锁表等，可以实时地查看 SQL 的执行情况，同时对一些锁表操作进行优化。 show processlist; 查询出来个列的含义 1） id列，用户登录mysql时，系统分配的\u0026#34;connection_id\u0026#34;，可以使用函数connection_id()查看 2） user列，显示当前用户。如果不是root，这个命令就只显示用户权限范围的sql语句 3） host列，显示这个语句是从哪个ip的哪个端口上发的，可以用来跟踪出现问题语句的用户 4） db列，显示这个进程目前连接的是哪个数据库 5） command列，显示当前连接的执行的命令，一般取值为休眠（sleep），查询（query），连接（connect）等 6） time列，显示这个状态持续的时间，单位是秒 7） state列，显示使用当前连接的sql语句的状态，很重要的列。state描述的是语句执行中的某一个状态。一个sql语句，以查询为例，可能需要经过copying to tmp table、sorting result、sending data等状态才可以完成 8） info列，显示这个sql语句，是判断问题语句的一个重要依据 explain分析执行计划 通过以上步骤查询到效率低的 SQL 语句后，可以通过 EXPLAIN或者 DESC命令获取 MySQL如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。 字段 含义 id select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。 select_type 表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（子查询中的第一个 SELECT）等 table 输出结果集的表 type 表示表的连接类型，性能由好到差的连接类型为( system \u0026mdash;\u0026gt; const \u0026mdash;\u0026ndash;\u0026gt; eq_ref \u0026mdash;\u0026mdash;\u0026gt; ref \u0026mdash;\u0026mdash;-\u0026gt; ref_or_null\u0026mdash;-\u0026gt; index_merge \u0026mdash;\u0026gt; index_subquery \u0026mdash;\u0026ndash;\u0026gt; range \u0026mdash;\u0026ndash;\u0026gt; index \u0026mdash;\u0026mdash;\u0026gt; all ) possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 rows 扫描行的数量 extra 执行情况的说明和描述 1.explain 之 id id 有相同，也有不同，同时存在。id相同的可以认为是一组，从上往下顺序执行；在所有的组中，id的值越大，优先级越高，越先执行。 2.explain 之 select_type select_type 含义 SIMPLE 简单的select查询，查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子查询，最外层查询标记为该标识 SUBQUERY 在SELECT 或 WHERE 列表中包含了子查询 DERIVED 在FROM 列表中包含的子查询，被标记为 DERIVED（衍生） MYSQL会递归执行这些子查询，把结果放在临时表中 UNION 若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED UNION RESULT 从UNION表获取结果的SELECT 3.explain 之 type type 显示的是访问类型，是较为重要的一个指标，可取值为： type 含义 NULL MySQL不访问任何表，索引，直接返回结果 system 表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现 const 表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常亮。const于将 \u0026ldquo;主键\u0026rdquo; 或 \u0026ldquo;唯一\u0026rdquo; 索引的所有部分与常量值进行比较 (使用唯一索引返回一条数据) eq_ref 类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描 (联表使用唯一索引返回一条数据) ref 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个） (使用索引返回结果) range 只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， \u0026lt; , \u0026gt; , in 等操作。 index index 与 ALL的区别为 index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。 all 将遍历全表以找到匹配的行 结果值从最好到最坏以此是：\nNULL \u0026gt; system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt; unique_subquery \u0026gt; index_subquery \u0026gt; range \u0026gt; index \u0026gt; ALL system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; range \u0026gt; index \u0026gt; ALL ==一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 。==\n4.explain 之 key possible_keys : 显示可能应用在这张表的索引， 一个或多个。 key ： 实际使用的索引， 如果为NULL， 则没有使用索引。 key_len : 表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。 5.explain 之 extra 其他的额外的执行计划信息，在该列展示。 extra 含义 using filesort 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取， 称为 “文件排序”, 效率低。 using temporary 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于 order by 和 group by； 效率低 using index 表示相应的select操作使用了覆盖索引， 避免访问表的数据行， 效率不错。 show profile分析SQL Mysql从5.0.37版本开始增加了对 show profiles 和 show profile 语句的支持。show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。 通过 have_profiling 参数，能够看到当前MySQL是否支持profile： select @@have_profiling; --查看状态 selecting @@profiling; --开启 set profiling = 1; --查看允许的sql语句 show profiles; --查看单个sql语句运行的时间 query_id为show profiles展示的id show profile for query query_id; 在获取到最消耗时间的线程状态后，MySQL支持进一步选择all、cpu、block io 、context switch、page faults等明细类型类查看MySQL在使用什么资源上耗费了过高的时间 --查看所有进程消耗的时间 show profile all for query query_id; --查看query_id的sql语句执行时cpu消耗的时间 show profile cpu for query query_id; MySQL5.6提供了对SQL的跟踪trace, 通过trace文件能够进一步了解为什么优化器选择A计划, 而不是选择B计划。 打开trace ， 设置格式为 JSON，并设置trace最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。 SET optimizer_trace=\u0026#34;enabled=on\u0026#34;,end_markers_in_json=on; set optimizer_trace_max_mem_size=1000000; 执行SQL语句 ： select * from test_user where id \u0026lt; 4; 最后， 检查information_schema.optimizer_trace就可以知道MySQL是如何执行SQL的 ： select * from information_schema.optimizer_trace\\G; ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emysql%E7%9A%84%E6%A6%82%E5%BF%B5/","summary":"索引 可以理解成:索引就是mysql高速获取数据的一种数据结构。 聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。 MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 B+树数据存叶子节点。索引存根节点。 优势 1） 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。 2） 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。 劣势 1） 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。 2） 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 索引设计原则 索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。 对查询频次较高，且数据量比较大的表建立索引。 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。 使用唯一索引，区分度越高，使用索引的效率越高。 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。 利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。 避免索引失效 全匹配原则。 最左前缀法则。 在最左前缀法则的基础上，范围查询右边的列，不会使用索引。 不要在索引列上进行运算操作， 索引将失效。 字符串类型不加单引号查询， 索引将失效。 尽量使用覆盖索引，避免使用select *（避免回表查询 什么是回表查询:mysql的索引是一个链子 在查找数据的时候找的是链子上面的索引，在找到链子的东西，select * 通过链子的索引的东西再去数据库表把整条找出来，这就叫索引回表。 避免回表查询：在建立复合索引时尽量把）； 避免or跟索引条件连用 如果仅仅是尾部模糊匹配（like \u0026lsquo;str%\u0026rsquo;），索引不会失效。如果是头部模糊匹配，索引失效。（用覆盖索引能避免模糊查询失效） 如果MySQL评估使用索引比全表更慢，则不使用索引。 单列索引和复合索引。尽量使用复合索引，而少使用单列索引 。 存储过程和函数 存储过程和函数概述 存储过程和函数是 事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 存储过程和函数的区别在于函数必须有返回值，而存储过程没有。 函数 ： 是一个有返回值的过程 ； 过程 ： 是一个没有返回值的函数 ； 各种存储引擎特性 下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ： 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 有 有 没有 有 事务安全 ==支持== 锁机制 ==行锁(适合高并发)== ==表锁== 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.","title":"关于mysql的部分概念"},{"content":"关于mongodb的权限问题 1. mongodb是没有默认管理员账号，所以要先添加管理员账号，在开启权限认证。 2. 切换到admin数据库，添加的账号才是管理员账号。 3. 用户只能在用户所在数据库登录，包括管理员账号。 4. 管理员可以管理所有数据库，但是不能直接管理其他数据库，要先在admin数据库认证后才可以 createUser的字段介绍 1. user字段，为新用户的名字； 2. pwd字段，用户的密码； 3. cusomData字段，为任意内容，例如可以为用户全名介绍，可不填； 4. roles字段，指定用户的角色，可以用一个空数组给新用户设定空角色； 5. 在roles字段,可以指定内置角色和用户定义的角色。 权限介绍 1. 数据库用户角色：read、readWrite; 2. 数据库管理角色：dbAdmin、dbOwner、userAdmin； 3. 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 4. 备份恢复角色：backup、restore； 5. 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 6. 超级用户角色：root // 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase） 7. 内部角色：__system 对应的权限 1. Read：允许用户读取指定数据库 2. readWrite：允许用户读写指定数据库 3. dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile 4. userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 5. clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 6. readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 7. readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 8. userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 9. dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 10. root：只在admin数据库中可用。超级账号，超级权限 创建用户的事例1 #进入表admin use admin #创建一个名为 admin，密码为 123456 的用户。 db.createUser({ user:\u0026#39;admin\u0026#39;,pwd:\u0026#39;123456\u0026#39;,roles:[ { role:\u0026#39;userAdminAnyDatabase\u0026#39;, db: \u0026#39;admin\u0026#39;},\u0026#34;readWriteAnyDatabase\u0026#34;]}); 创建用户的事咧2 #进入表test use test #在test数据库创建用户test，并给该用户admin数据库上clusterAdmin和readAnyDatabase的角色，test数据库上readWrite角色。 db.createUser( { \u0026#34;user\u0026#34; : \u0026#34;test\u0026#34;, \u0026#34;pwd\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;roles\u0026#34; : [ { role: \u0026#34;clusterAdmin\u0026#34;, db: \u0026#34;admin\u0026#34; }, { role: \u0026#34;readAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34; }, \u0026#34;readWrite\u0026#34; ] }, { w: \u0026#34;majority\u0026#34; , wtimeout: 5000 } ) mac启动mongodb brew services stop mongodb-community@4.4 参考链接 https://www.cnblogs.com/yucongblog/p/6595946.html ","permalink":"http://localhost:1313/2021/%E5%85%B3%E4%BA%8Emongodb/","summary":"关于mongodb的权限问题 1. mongodb是没有默认管理员账号，所以要先添加管理员账号，在开启权限认证。 2. 切换到admin数据库，添加的账号才是管理员账号。 3. 用户只能在用户所在数据库登录，包括管理员账号。 4. 管理员可以管理所有数据库，但是不能直接管理其他数据库，要先在admin数据库认证后才可以 createUser的字段介绍 1. user字段，为新用户的名字； 2. pwd字段，用户的密码； 3. cusomData字段，为任意内容，例如可以为用户全名介绍，可不填； 4. roles字段，指定用户的角色，可以用一个空数组给新用户设定空角色； 5. 在roles字段,可以指定内置角色和用户定义的角色。 权限介绍 1. 数据库用户角色：read、readWrite; 2. 数据库管理角色：dbAdmin、dbOwner、userAdmin； 3. 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 4. 备份恢复角色：backup、restore； 5. 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 6. 超级用户角色：root // 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase） 7. 内部角色：__system 对应的权限 1. Read：允许用户读取指定数据库 2. readWrite：允许用户读写指定数据库 3. dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile 4. userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 5. clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 6. readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 7. readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 8. userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 9. dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 10. root：只在admin数据库中可用。超级账号，超级权限 创建用户的事例1 #进入表admin use admin #创建一个名为 admin，密码为 123456 的用户。 db.createUser({ user:\u0026#39;admin\u0026#39;,pwd:\u0026#39;123456\u0026#39;,roles:[ { role:\u0026#39;userAdminAnyDatabase\u0026#39;, db: \u0026#39;admin\u0026#39;},\u0026#34;readWriteAnyDatabase\u0026#34;]}); 创建用户的事咧2 #进入表test use test #在test数据库创建用户test，并给该用户admin数据库上clusterAdmin和readAnyDatabase的角色，test数据库上readWrite角色。 db.","title":"关于mongoDB权限问题"},{"content":"查看Centos系统是否自带mysql安装包 rpm -qa ｜ grep -i mysql 卸载Centos系统是否自带mysql安装包 rpm -e 安装包名称 --nodeps ","permalink":"http://localhost:1313/2021/linux%E4%B8%8A%E4%BD%BF%E7%94%A8mysql/","summary":"查看Centos系统是否自带mysql安装包 rpm -qa ｜ grep -i mysql 卸载Centos系统是否自带mysql安装包 rpm -e 安装包名称 --nodeps ","title":"linux上使用mysql"},{"content":"数据库表的准确值（缺点在数据量很大的时候即使加了索引 查询所需时间依旧比较长） SELECT count(1) as total FROM test_user; table_rows只是估算值，比起count，该查询所需时间极少，对于千万级别数据量的表（大约1s左右），很有用的 #TABLE_NAME为表名 TABLE_SCHEMA为数据库名 select TABLE_ROWS from information_schema.TABLES where TABLE_NAME=\u0026#39;test_user\u0026#39; AND TABLE_SCHEMA = \u0026#39;TestMysql\u0026#39;; ","permalink":"http://localhost:1313/2021/mysql-count-1-%E4%B8%8Eselect-table-rows-from-information-schema%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"数据库表的准确值（缺点在数据量很大的时候即使加了索引 查询所需时间依旧比较长） SELECT count(1) as total FROM test_user; table_rows只是估算值，比起count，该查询所需时间极少，对于千万级别数据量的表（大约1s左右），很有用的 #TABLE_NAME为表名 TABLE_SCHEMA为数据库名 select TABLE_ROWS from information_schema.TABLES where TABLE_NAME=\u0026#39;test_user\u0026#39; AND TABLE_SCHEMA = \u0026#39;TestMysql\u0026#39;; ","title":"mysql count(1)与select TABLE_ROWS from information_schema的区别"},{"content":"创建内存表 CREATE TABLE `test_user_memory` ( `id` int(11) NOT NULL AUTO_INCREMENT comment \u0026#39;主键id\u0026#39;, `user_id` varchar(36) NOT NULL comment \u0026#39;用户id\u0026#39;, `user_name` varchar(30) NOT NULL comment \u0026#39;用户名称\u0026#39;, `phone` varchar(20) NOT NULL comment \u0026#39;手机号码\u0026#39;, `lan_id` int(9) NOT NULL comment \u0026#39;本地网\u0026#39;, `region_id` int(9) NOT NULL comment \u0026#39;区域\u0026#39;, `create_time` datetime NOT NULL comment \u0026#39;创建时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_user_id` (`user_id`) ) ENGINE=MEMORY DEFAULT CHARSET=utf8mb4; 创建普通表 CREATE TABLE `test_user` ( `id` int(11) NOT NULL AUTO_INCREMENT comment \u0026#39;主键id\u0026#39;, `user_id` varchar(36) NOT NULL comment \u0026#39;用户id\u0026#39;, `user_name` varchar(30) NOT NULL comment \u0026#39;用户名称\u0026#39;, `phone` varchar(20) NOT NULL comment \u0026#39;手机号码\u0026#39;, `lan_id` int(9) NOT NULL comment \u0026#39;本地网\u0026#39;, `region_id` int(9) NOT NULL comment \u0026#39;区域\u0026#39;, `create_time` datetime NOT NULL comment \u0026#39;创建时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_user_id` (`user_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 生成手机号码的时候要用到 生成n个随机数字 DELIMITER $$ CREATE FUNCTION randNum(n int) RETURNS VARCHAR(255) BEGIN DECLARE chars_str varchar(20) DEFAULT \u0026#39;0123456789\u0026#39;; DECLARE return_str varchar(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str = concat(return_str,substring(chars_str , FLOOR(1 + RAND()*10 ),1)); SET i = i +1; END WHILE; RETURN return_str; END $$ 创建生成号码函数 #生成随机手机号码 #定义常用的手机头 130 131 132 133 134 135 136 137 138 139 186 187 189 151 157 #SET starts = 1+floor(rand()*15)*4; 截取字符串的开始是从 1、5、9、13 ...开始的。floor(rand()*15)的取值范围是0~14 #SET head = substring(bodys,starts,3);在字符串bodys中从starts位置截取三位 DELIMITER $$ CREATE FUNCTION generatePhone() RETURNS varchar(20) BEGIN DECLARE head char(3); DECLARE phone varchar(20); DECLARE bodys varchar(100) default \u0026#34;130 131 132 133 134 135 136 137 138 139 186 187 189 151 157\u0026#34;; DECLARE starts int; SET starts = 1+floor(rand()*15)*4; SET head = trim(substring(bodys,starts,3)); SET phone = trim(concat(head,randNum(8))); RETURN phone; END $$ 创建随机字符串和随机时间的函数 DELIMITER $$ CREATE FUNCTION `randStr`(n INT) RETURNS varchar(255) CHARSET utf8mb4 DETERMINISTIC BEGIN DECLARE chars_str varchar(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\u0026#39;; DECLARE return_str varchar(255) DEFAULT \u0026#39;\u0026#39; ; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str = concat(return_str, substring(chars_str, FLOOR(1 + RAND() * 62), 1)); SET i = i + 1; END WHILE; RETURN return_str; END$$ DELIMITER; 创建插入内存表数据存储过程 入参n是多少就插入多少条数据 DELIMITER $$ CREATE PROCEDURE `add_test_user_memory`(IN n int) BEGIN DECLARE i INT DEFAULT 1; WHILE (i \u0026lt;= n) DO INSERT INTO test_user_memory (user_id, user_name, phone, lan_id,region_id, create_time) VALUES (uuid(), randStr(20), generatePhone(), FLOOR(RAND() * 1000), FLOOR(RAND() * 100), NOW()); SET i = i + 1; END WHILE; END $$ DELIMITER ; 循环从内存表获取数据插入普通表 #参数描述 n表示循环调用几次；count表示每次插入内存表和普通表的数据量 DELIMITER $$ CREATE PROCEDURE `add_test_user_memory_to_outside`(IN n int, IN count int) BEGIN DECLARE i INT DEFAULT 1; WHILE (i \u0026lt;= n) DO CALL add_test_user_memory(count); INSERT INTO test_user SELECT * FROM test_user_memory; delete from test_user_memory; SET i = i + 1; END WHILE; END $$ DELIMITER ; 先调用存储过程往内存表插入一万条数据，然后再把内存表的一万条数据插入普通表 CALL add_test_user_memory(100000); 一次性把内存表的数据插入到普通表，这个过程是很快的 INSERT INTO test_user SELECT * FROM test_user_memory; 清空内存表数据 delete from test_user_memory; 设置表ID的起始值 alter table test_user_memory AUTO_INCREMENT=5040137; 清空表 truncate test_user_memory; truncate test_user; 循环100次，每次生成10000条数据 总共生成一百万条数据 CALL add_test_user_memory_to_outside(100,10000); 修改mysql内存表存储大小的值 通过执行mysql命令修改 SET GLOBAL tmp_table_size=2147483648; SET GLOBAL max_heap_table_size=2147483648; ","permalink":"http://localhost:1313/2021/mysql-%E8%84%9A%E6%9C%AC%E7%94%9F%E5%AD%98%E7%99%BE%E4%B8%87%E8%B7%B3%E6%95%B0%E6%8D%AE/","summary":"创建内存表 CREATE TABLE `test_user_memory` ( `id` int(11) NOT NULL AUTO_INCREMENT comment \u0026#39;主键id\u0026#39;, `user_id` varchar(36) NOT NULL comment \u0026#39;用户id\u0026#39;, `user_name` varchar(30) NOT NULL comment \u0026#39;用户名称\u0026#39;, `phone` varchar(20) NOT NULL comment \u0026#39;手机号码\u0026#39;, `lan_id` int(9) NOT NULL comment \u0026#39;本地网\u0026#39;, `region_id` int(9) NOT NULL comment \u0026#39;区域\u0026#39;, `create_time` datetime NOT NULL comment \u0026#39;创建时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_user_id` (`user_id`) ) ENGINE=MEMORY DEFAULT CHARSET=utf8mb4; 创建普通表 CREATE TABLE `test_user` ( `id` int(11) NOT NULL AUTO_INCREMENT comment \u0026#39;主键id\u0026#39;, `user_id` varchar(36) NOT NULL comment \u0026#39;用户id\u0026#39;, `user_name` varchar(30) NOT NULL comment \u0026#39;用户名称\u0026#39;, `phone` varchar(20) NOT NULL comment \u0026#39;手机号码\u0026#39;, `lan_id` int(9) NOT NULL comment \u0026#39;本地网\u0026#39;, `region_id` int(9) NOT NULL comment \u0026#39;区域\u0026#39;, `create_time` datetime NOT NULL comment \u0026#39;创建时间\u0026#39;, PRIMARY KEY (`id`), KEY `idx_user_id` (`user_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 生成手机号码的时候要用到 生成n个随机数字 DELIMITER $$ CREATE FUNCTION randNum(n int) RETURNS VARCHAR(255) BEGIN DECLARE chars_str varchar(20) DEFAULT \u0026#39;0123456789\u0026#39;; DECLARE return_str varchar(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str = concat(return_str,substring(chars_str , FLOOR(1 + RAND()*10 ),1)); SET i = i +1; END WHILE; RETURN return_str; END $$ 创建生成号码函数 #生成随机手机号码 #定义常用的手机头 130 131 132 133 134 135 136 137 138 139 186 187 189 151 157 #SET starts = 1+floor(rand()*15)*4; 截取字符串的开始是从 1、5、9、13 .","title":"mysql 脚本生成百万条数据"},{"content":"crontab -l 查看定时任务 crontab -e 编辑定时任务 写法 (定时任务只会执行一次) #每年4月 7月 10月 1月一日凌晨00:10执行 #/usr/bin/php 表示用/usr/bin/php这个位置的php去解析后面的/data/mxjk/cli/quarter_report.php 的php文件 10 00 1 1,4,7,10 * /usr/bin/php /data/mxjk/cli/quarter_report.php #每年4月 7月 10月 1月一日晚上8:00向用户推送报告 00 20 1 1,4,7,10 * /usr/bin/php /data/mxjk/cli/push_report.php #每日00:10统计设备检测数 10 00 * * * curl https://www.maixiangjk.com/api/test_blue/upToday 参考链接 https://www.cnblogs.com/dabai-wang09/p/11260766.html ","permalink":"http://localhost:1313/2021/centos%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","summary":"crontab -l 查看定时任务 crontab -e 编辑定时任务 写法 (定时任务只会执行一次) #每年4月 7月 10月 1月一日凌晨00:10执行 #/usr/bin/php 表示用/usr/bin/php这个位置的php去解析后面的/data/mxjk/cli/quarter_report.php 的php文件 10 00 1 1,4,7,10 * /usr/bin/php /data/mxjk/cli/quarter_report.php #每年4月 7月 10月 1月一日晚上8:00向用户推送报告 00 20 1 1,4,7,10 * /usr/bin/php /data/mxjk/cli/push_report.php #每日00:10统计设备检测数 10 00 * * * curl https://www.maixiangjk.com/api/test_blue/upToday 参考链接 https://www.cnblogs.com/dabai-wang09/p/11260766.html ","title":"centos定时任务"},{"content":"Docker常用命令 拉取镜像： docker pull [镜像名/版本号] 搜索镜像： docker search [镜像名] 查看镜像： docker images [-a 所有images /f 过滤（使用较少）/q 只显示images ID] 删除镜像： docker rmi [镜像ID/镜像名] 删除所有镜像： docker rmi $(docker images -q) 启动容器： docker run -d -i -t [容器ID] /bin/bash 进入当前运行的容器： docker exec -it [容器ID] /bin/bash 查看容器详细信息： docker inspect [容器ID] 查看容器： docker ps [-a/f/n/l/q/s] 从容器内拷贝文件到主机上： docker cp [容器ID] 退出不停止容器： control + p + q 关闭容器： docker stop 容器ID 启动容器： docker start 容器ID 重启容器： docker restart 容器ID 删删除所有容器： docker rm $(docker ps -aq) 拉取php镜像 #拉取php镜像 docker pull php:7.4-fpm 拉取nginx镜像 #拉取php镜像 docker pull nginx 拉取mysql镜像 #拉取php镜像 docker pull mysql/mysql-server:5.7 #创建mysql容器 docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql/mysql-server:5.7 #查看所有的容器 docker pa -a #运行容器 docker start [容器ID] #进入mysql 修改登陆用户为root #因为在数据库服务器中的mysql数据库中的user的表中没有权限(也可以说没有用户)，user表中Host是localhost，所以需要执行以下操作将Host改为通配符’%’： #（1） docker exec -it mysql bash #（2） mysql -u root -p #show databases; #use mysql; #show tables; #select Host, User,Password from user; #（3） update user set Host=\u0026#39;%\u0026#39; where User=\u0026#39;root\u0026#39;; #（4） flush privileges; #在退出docker数据库后 查看docker主机ip ifconfig | grep inet #然后使用Navicat即可链接 ","permalink":"http://localhost:1313/2021/docker%E5%85%A5%E9%97%A8/","summary":"Docker常用命令 拉取镜像： docker pull [镜像名/版本号] 搜索镜像： docker search [镜像名] 查看镜像： docker images [-a 所有images /f 过滤（使用较少）/q 只显示images ID] 删除镜像： docker rmi [镜像ID/镜像名] 删除所有镜像： docker rmi $(docker images -q) 启动容器： docker run -d -i -t [容器ID] /bin/bash 进入当前运行的容器： docker exec -it [容器ID] /bin/bash 查看容器详细信息： docker inspect [容器ID] 查看容器： docker ps [-a/f/n/l/q/s] 从容器内拷贝文件到主机上： docker cp [容器ID] 退出不停止容器： control + p + q 关闭容器： docker stop 容器ID 启动容器： docker start 容器ID 重启容器： docker restart 容器ID 删删除所有容器： docker rm $(docker ps -aq) 拉取php镜像 #拉取php镜像 docker pull php:7.","title":"docker入门"},{"content":"PHP 后段代码 1.校验微信JSSDK 具体代码 微信公众号h5跳转小程序的ticket是用微信公众号的access_token生成的（小程序这边的ticket都是用微信公众号的appid跟密钥） \u0026lt;?php namespace app\\wechat\\controller; use think\\Controller; use think\\Db; use app\\wechat\\controller\\Token; define(\u0026#39;APPID\u0026#39;, \u0026#39;微信公众号appid\u0026#39;); define(\u0026#39;APPSERCET\u0026#39;, \u0026#39;微信公众号密钥\u0026#39;); Class WxMini extends Controller{ public static function http_request($url,$data = null){ $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, FALSE); curl_setopt($curl, CURLOPT_SSL_VERIFYHOST, FALSE); if (!empty($data)){ curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, $data); } curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); $output = curl_exec($curl); curl_close($curl); return $output; } public function getTicket(){ //获取ticket $Ticket = Db::table(\u0026#34;mxjk_access_token\u0026#34;)-\u0026gt;where([\u0026#39;name\u0026#39;=\u0026gt;\u0026#34;ticket\u0026#34;,\u0026#39;id\u0026#39;=\u0026gt;2])-\u0026gt;find(); $now = time(); if($now \u0026gt; $Ticket[\u0026#39;expires_in\u0026#39;]){ //重新申请token $token = new Token(); $access_token = $token-\u0026gt;getAccessToken(); $url = \u0026#34;https://api.weixin.qq.com/cgi-bin/ticket/getticket?access_token=\u0026#34;.$access_token.\u0026#34;\u0026amp;type=jsapi\u0026#34;; $res = $this-\u0026gt;http_request($url); $res = json_decode($res,true); //得到token存数据库 Db::table(\u0026#34;mxjk_access_token\u0026#34;)-\u0026gt;where([\u0026#39;id\u0026#39;=\u0026gt;2,\u0026#39;name\u0026#39;=\u0026gt;\u0026#34;ticket\u0026#34;])-\u0026gt;update([ \u0026#39;access_token\u0026#39; =\u0026gt; $res[\u0026#39;ticket\u0026#39;], \u0026#39;expires_in\u0026#39; =\u0026gt; time()+7000, \u0026#39;update_time\u0026#39; =\u0026gt; time(), ]); return $res[\u0026#39;ticket\u0026#39;]; } return $Ticket[\u0026#39;access_token\u0026#39;]; } public function randomkeys($length){ $key = \u0026#39;\u0026#39;; $pattern = \u0026#39;1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLOMNOPQRSTUVWXYZ\u0026#39;; for($i=0;$i\u0026lt;$length;$i++) { $key .= $pattern{mt_rand(0,35)}; //生成php随机数 } return $key; } public function GetSign(){ $url = $_POST[\u0026#39;url\u0026#39;]; //时间戳 $nTime = time(); //随机字符串 $sStr = $this-\u0026gt;randomkeys(16); //拼接字符串 $ticket = $this-\u0026gt;getTicket(); //这里参数的顺序要按照 key值 ASCII 码升序排序j -\u0026gt; n -\u0026gt; t -\u0026gt; u $sSign = \u0026#34;jsapi_ticket=\u0026#34;.$ticket.\u0026#34;\u0026amp;noncestr=\u0026#34;.$sStr.\u0026#34;\u0026amp;timestamp=\u0026#34;.$nTime.\u0026#34;\u0026amp;url=\u0026#34;.$url; $sSigns = sha1($sSign);//签名 //需要提供给前端的数据 $sData = array( \u0026#39;appId\u0026#39; =\u0026gt;\u0026#39;wx62c735d31110c6d0\u0026#39;,//公众号的唯一标识 \u0026#39;timestamp\u0026#39;=\u0026gt;$nTime, //生成签名的时间戳 \u0026#39;nonceStr\u0026#39; =\u0026gt;$sStr, //生成签名的随机串 \u0026#39;signature\u0026#39;=\u0026gt;$sSigns //签名 ); return $sData; } public function Index(){ return view(\u0026#39;\u0026#39;); } } 前段 代码 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;meta content=\u0026#34;width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0\u0026#34; name=\u0026#34;viewport\u0026#34; /\u0026gt; \u0026lt;meta content=\u0026#34;yes\u0026#34; name=\u0026#34;apple-mobile-web-app-capable\u0026#34;\u0026gt; \u0026lt;meta content=\u0026#34;black\u0026#34; name=\u0026#34;apple-mobile-web-app-status-bar-style\u0026#34;\u0026gt; \u0026lt;meta content=\u0026#34;telephone=no\u0026#34; name=\u0026#34;format-detection\u0026#34;\u0026gt; \u0026lt;meta content=\u0026#34;yes\u0026#34; name=\u0026#34;apple-touch-fullscreen\u0026#34;\u0026gt; \u0026lt;!--//引入微信js--\u0026gt; \u0026lt;script src=\u0026#34;https://res2.wx.qq.com/open/js/jweixin-1.6.0.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!--//username为appid的原始id path为页面路径 可?id=xxx\u0026amp;name=xxx附带参数给小程序--\u0026gt; \u0026lt;wx-open-launch-weapp id=\u0026#34;launch-btn\u0026#34; username=\u0026#34;gh_1dac5028a5dd\u0026#34; path=\u0026#34;/pages/index/index.html\u0026#34; style=\u0026#34;display: block;width: 100%;\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;text/wxtag-template\u0026#34;\u0026gt; \u0026lt;style\u0026gt;.btn{ padding: 12px }\u0026lt;/style\u0026gt; \u0026lt;button class=\u0026#34;btn\u0026#34;\u0026gt;按钮名称\u0026lt;/button\u0026gt; \u0026lt;/script \u0026gt; \u0026lt;/wx-open-launch-weapp\u0026gt; \u0026lt;script src=\u0026#34;/wx/bindphone/js/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; $(function(){ var url = location.href.split(\u0026#39;#\u0026#39;)[0];//获取当前页面的url console.log(url); //GetSign为后端接口 $.post(\u0026#34;GetSign\u0026#34;, {\u0026#39;url\u0026#39;: url}, function (result) { console.log(result.appId); wx.config({ debug: true, // 开启调试模式,调用的所有api的返回值会在客户端alert出来，若要查看传入的参数，可以在pc端打开，参数信息会通过log打出，仅在pc端时才会打印 appId: result.appId, // 必填，公众号的唯一标识 timestamp: result.timestamp, // 必填，生成签名的时间戳 nonceStr: result.nonceStr, // 必填，生成签名的随机串 signature: result.signature,// 必填，签名 jsApiList: [\u0026#39;openLocation\u0026#39;], // 必填，需要使用的JS接口列表 openTagList: [\u0026#39;wx-open-launch-weapp\u0026#39;] // 可选，需要使用的开放标签列表，例如[\u0026#39;wx-open-launch-app\u0026#39;] }); wx.ready(function () { // config信息验证后会执行ready方法，所有接口调用都必须在config接口获得结果之后，config是一个客户端的异步操作，所以如果需要在页面加载时就调用相关接口，则须把相关接口放在ready函数中调用来确保正确执行。对于用户触发时才调用的接口，则可以直接调用，不需要放在ready函数中 }); wx.error(function (res) { // config信息验证失败会执行error函数，如签名过期导致验证失败，具体错误信息可以打开config的debug模式查看，也可以在返回的res参数中查看，对于SPA可以在这里更新签名 }); var btn = document.getElementById(\u0026#39;launch-btn\u0026#39;); btn.addEventListener(\u0026#39;launch\u0026#39;, function (e) { console.log(\u0026#39;success\u0026#39;); }); btn.addEventListener(\u0026#39;error\u0026#39;, function (e) { console.log(\u0026#39;fail\u0026#39;, e.detail); }); }); }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","permalink":"http://localhost:1313/2021/h5%E8%B7%B3%E8%BD%AC%E5%B0%8F%E7%A8%8B%E5%BA%8F/","summary":"PHP 后段代码 1.校验微信JSSDK 具体代码 微信公众号h5跳转小程序的ticket是用微信公众号的access_token生成的（小程序这边的ticket都是用微信公众号的appid跟密钥） \u0026lt;?php namespace app\\wechat\\controller; use think\\Controller; use think\\Db; use app\\wechat\\controller\\Token; define(\u0026#39;APPID\u0026#39;, \u0026#39;微信公众号appid\u0026#39;); define(\u0026#39;APPSERCET\u0026#39;, \u0026#39;微信公众号密钥\u0026#39;); Class WxMini extends Controller{ public static function http_request($url,$data = null){ $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, FALSE); curl_setopt($curl, CURLOPT_SSL_VERIFYHOST, FALSE); if (!empty($data)){ curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, $data); } curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); $output = curl_exec($curl); curl_close($curl); return $output; } public function getTicket(){ //获取ticket $Ticket = Db::table(\u0026#34;mxjk_access_token\u0026#34;)-\u0026gt;where([\u0026#39;name\u0026#39;=\u0026gt;\u0026#34;ticket\u0026#34;,\u0026#39;id\u0026#39;=\u0026gt;2])-\u0026gt;find(); $now = time(); if($now \u0026gt; $Ticket[\u0026#39;expires_in\u0026#39;]){ //重新申请token $token = new Token(); $access_token = $token-\u0026gt;getAccessToken(); $url = \u0026#34;https://api.","title":"h5跳转小程序"},{"content":"创建用户并配置其仓库 useradd git #这样服务器home目录里就会多一个git用户 passwd git #设置密码 su git #这步很重要，不切换用户后面会很麻烦 cd /2021_my/myblog #进入部署的文件夹 mkdir blog #项目存在的真实目录 mkdir temp #这个待会用来放缓存 mkdir repos \u0026amp;\u0026amp; cd repos git init --bare blog.git #在repos下创建一个裸露的仓库 cd blog.git/hooks #进入该目录 配置git vi post-receive #创建hook钩子函数，输入了内容如下 #!/bin/sh GIT_REPO=/2021_my/myblog/repos/blog.git #git配置项 TMP_GIT_CLONE=/2021_my/myblog/temp #缓存 PUBLIC_WWW=/2021_my/myblog/blog #自己博客的代码 rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE rm -rf ${PUBLIC_WWW} cp -rf ${TMP_GIT_CLONE} ${PUBLIC_WWW} git用户关于post-receive脚本及内容的权限、虚拟主机的权限 #赋予文件、文件夹权限 chmod 777 post-receive chmod 777 -R /2021_my/myblog/temp chmod 777 -R /2021_my/myblog/blog #赋予git用户权限 chown git:git -R /2021_my/myblog/temp chown git:git -R /2021_my/myblog/blog 本机Hexo配置 deploy: type: git repo: git@服务器地址:/2021_my/myblog/repos/blog.git branch: master 本地上传 hexo clean \u0026amp;\u0026amp; hexo g \u0026amp;\u0026amp; hexo d 创建新的文件 hexo new \u0026#34;文件名称\u0026#34; ","permalink":"http://localhost:1313/2021/hexo%E9%83%A8%E7%BD%B2%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"创建用户并配置其仓库 useradd git #这样服务器home目录里就会多一个git用户 passwd git #设置密码 su git #这步很重要，不切换用户后面会很麻烦 cd /2021_my/myblog #进入部署的文件夹 mkdir blog #项目存在的真实目录 mkdir temp #这个待会用来放缓存 mkdir repos \u0026amp;\u0026amp; cd repos git init --bare blog.git #在repos下创建一个裸露的仓库 cd blog.git/hooks #进入该目录 配置git vi post-receive #创建hook钩子函数，输入了内容如下 #!/bin/sh GIT_REPO=/2021_my/myblog/repos/blog.git #git配置项 TMP_GIT_CLONE=/2021_my/myblog/temp #缓存 PUBLIC_WWW=/2021_my/myblog/blog #自己博客的代码 rm -rf ${TMP_GIT_CLONE} git clone $GIT_REPO $TMP_GIT_CLONE rm -rf ${PUBLIC_WWW} cp -rf ${TMP_GIT_CLONE} ${PUBLIC_WWW} git用户关于post-receive脚本及内容的权限、虚拟主机的权限 #赋予文件、文件夹权限 chmod 777 post-receive chmod 777 -R /2021_my/myblog/temp chmod 777 -R /2021_my/myblog/blog #赋予git用户权限 chown git:git -R /2021_my/myblog/temp chown git:git -R /2021_my/myblog/blog 本机Hexo配置 deploy: type: git repo: git@服务器地址:/2021_my/myblog/repos/blog.","title":"hexo部署在自己的服务器"},{"content":"nginx 第一步 直接yum yum install nginx 如果是虚拟机则可能会说找不到nginx的包运行如下命令 #rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm cd /etc/yum.repos.d/rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm yum update #更新yum yum install nginx 第二步 关于nginx的启动 systemctl start nginx #启动 systemctl restart nginx #重启 systemctl stop nginx #关闭 systemctl reload nginx #重载nginx配置 安装差异 nginx -s reload(配置了全局变量) systemctl status nginx #查看运行状况 PHP 第一步 装epel源 sudo dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.3.noarch.rpm 第二步 安装yum源管理工具 yum install yum-utils #sudo dnf -y install yum-utils 如果是虚拟机在此之前可能需要如下命令 yum install epel- release -y yum install dnf 第三步 sudo dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.3.noarch.rpm sudo dnf -y install https://rpms.remirepo.net/enterprise/remi-release-7.3.rpm #如需移除则用remove 第四步 sudo dnf install php -y #（默认下载php5.4.16） #yum install php73 #启动 systemctl status php73-php-fpm php -v #查看版本有些时候因为下载问题则需要php73 -v 阿蓝教我的另一个安装php7.3 #安装EPEL 源 yum install epel-release#安装REMI 源 yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm #安装Yum 源管理工具 yum install yum-utils#安装 PHP7.3 yum install -y php73-php-fpm php73-php-cli php73-php-bcmath php73-php-gd php73-php-json php73-php-mbstring php73-php-mcrypt php73-php-mysqlnd php73-php-opcache php73-php-pdo php73-php-pecl-crypto php73-php-pecl-mcrypt php73-php-pecl-geoip php73-php-pecl-swoole php73-php-recode php73-php-snmp php73-php-soap php73-php-xmll #设置开机自动启动 systemctl enable php73-php-fpm #启动服务 systemctl start php73-php-fpm #查看服务状态 systemctl status php73-php-fpm mysql 第一步 yum install mysql-server 虚拟机可能会报错 则需如下 #执行报错yum找不到包 就执行下面的语句 #rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm cd /etc/yum.repos.d/rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm yum update 第二步 放开权限 chown mysql:mysql -R /var/lib/mysql mysqld --initialize 第三步 启动 systemctl start mysqld systemctl status mysqld 过程中可能遇到的问题 #问题1：mysql启动失败 报错如下 [root@iZbp1foad7v3ogkpwab7yzZ ~]# systemctl start mysqld Job for mysqld.service failed because the control process exited with error code. See \u0026#34;systemctl status mysqld.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. #解决方案： #（1）查看mysql的日志vim /var/log/mysqld.log #（2）发现文件权限不可写 #执行 chown mysql:mysql -R /var/lib/mysql #然后再启动：systemctl start mysqld #发现启动成功 #问题2：myqsl -uroot -p linux mysql连接不上 报错 ERROR 1045 (28000): Access denied for user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; (using password: NO) 或Access denied for user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; (using password: YES) 解决： #停止mysql service mysqld stop #找到my.cnf文件 修改配置文件无密码登录 vi /etc/my.cnf #在文件[mysqld]后任意一行加入 （如果没找到该参数，看看有没有include其他文件 去其他文件里看看 都没有则直接加入）skip-grant-tables #启动mysql systemctl start mysqld #然后执行mysql mysql #设置密码SET PASSWORD FOR root@localhost = \u0026#39;123456\u0026#39;; #如果报错ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement #先执行 ：flush privileges; #然后再设置密码：SET PASSWORD FOR root@localhost = \u0026#39;123456\u0026#39;; #然后再：flush privileges; #设置好密码之后将 添加在文件[mysqld]后任意一行加入的skip-grant-tables去掉 #如果又连不上，可以重启mysql再试试：systemctl restart mysqld #问题3：navicate远程连接报错 #2013 - Lost connection to MySQL server at \u0026#39;reading initial communication packet\u0026#39;, system error: 0 \u0026#34;Internal error/check (Not system error)\u0026#34; #解决方案： #之前在/etc/my.cnf文件[mysqld]加入 的skip-grant-tables去掉，然后重新启动mysql就可以了 ","permalink":"http://localhost:1313/2021/yum%E5%AE%89%E8%A3%85lnmp/","summary":"nginx 第一步 直接yum yum install nginx 如果是虚拟机则可能会说找不到nginx的包运行如下命令 #rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm cd /etc/yum.repos.d/rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm yum update #更新yum yum install nginx 第二步 关于nginx的启动 systemctl start nginx #启动 systemctl restart nginx #重启 systemctl stop nginx #关闭 systemctl reload nginx #重载nginx配置 安装差异 nginx -s reload(配置了全局变量) systemctl status nginx #查看运行状况 PHP 第一步 装epel源 sudo dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.3.noarch.rpm 第二步 安装yum源管理工具 yum install yum-utils #sudo dnf -y install yum-utils 如果是虚拟机在此之前可能需要如下命令 yum install epel- release -y yum install dnf 第三步 sudo dnf -y install https://dl.","title":"yum安装lnmp"},{"content":"lnmp 第一步创建一个总文件夹 mkdir /data/{soft,server} -p nginx 第一步 useradd www -s /sbin/nologin -M 第二步 从nginx官网下载它的包 wget http://nginx.org/download/nginx-1.6.3.tar.gz 第三步 解压nginx tar xzf nginx (tar xzf nginx-1.6.3.tar.gz) 第四步 将解压出来的nginx 放到指定目录 ./configure --prefix=/data/server/nginx] (cd nginx-1.6.3再执行该命令 linux环境下安装软件是./configure编译文件 --prefix 到指定目录) 在此期间如有报错 需要用yum安装的命令 [./configure: error: C compiler cc is not found] yum -y install gcc [he HTTP rewrite module requires the PCRE library.] yum -y install pcre-devel [./configure: error: the HTTP gzip module requires the zlib library.] yum install -y zlib-devel 第五步 编译安装 make \u0026amp;\u0026amp; make install 第六步 修改nginx的配置文件 vim /data/server/nginx/conf/nginx.conf 修改第一行 user nobody 为 user www; 第七步 关于nginx的启动 启动 ./sbin/nginx 关闭 ./sbin/nginx -s stop 重启 ./sbin/nginx -s reload 第八步 配置nginx环境变量 添加Nginx环境变量，可以在命令行直接输入Nginx命令 vim /etc/profile #在最后添加Nginx的路径 export NGINX_HOME=/data/server/nginx (此处为安装的nginx的路径) export PATH=$PATH:$NGINX_HOME/sbin #重新编译环境变量 source /etc/profile mysql 第一步 useradd -s /sbin/nologin -M mysql 第二步 在官网下载mysql wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.26-el7-x86_64.tar.gz 第三部 解压下载的mysql的压缩包 并安装 tar xzf mysql-8.0.26-el7-x86_64.tar.gz -C /data/server cd /data/server #改名 mv mysql-8.0.26-el7-x86_64 mysql cd mysql #bin/mysqld --initialize --user=mysql --basedir= 安装目录 --datadir= 数据存放目录 bin/mysqld --initialize --user=mysql --basedir=/data/server/mysql --datadir=/data/server/mysql/data #这里会有密码 需要记住密码 大概会如下所示 root@localhost: brQaLw.Qu6bV 第四步 编辑mysql的配置文件 文件需修改的地方如下所示 vim /etc/my.cnf [mysqld] basedir = /data/server/mysql datadir=/data/server/mysql/data #socket=/data/server/mysql/mysql.sock character-set-server=utf8 port=3306 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 # [client] # # 设置mysql客户端连接服务端时默认使用的端口和默认字符集 port=3306 default-character-set=utf8 # Disabling symbolic-links is recommended to prevent assorted security risks # symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd [mysqld_safe] #log-error=/var/log/mariadb/mariadb.log #pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d 第五步 开通mysql的权限 chown -R mysql.mysql /data/server/mysql/ 第六步 数据库启动以及修改数据库密码 #启动 cd /support-files/mysql.server ./mysql.server start #进入mysql cd /mysql/bin/ ./mysql -uroot -p #再输入密码即可 show databases #修改密码 alter user user() identified by \u0026#34;123456\u0026#34;; #想要修改的密码 quit; #退出 第七步 给mysql配置环境变量 export PATH=/data/server/mysql/bin:$PATH PHP 第一步 下载php的包 wget https://www.php.net/distributions/php-7.4.22.tar.gz 第二步 解压压缩包 tar xzf php-7.4.22.tar.gz cd php-7.4.22 #准备配置项 ./configure --prefix=/data/server/php74 --enable-fpm --with-mysqli --with-curl --with-pdo_mysql --with-pdo_sqlite --enable-mysqlnd --enable-mbstring 中间可能有问题 需要下载的各种依赖 //从源代码编译 //编译 PHP 时需要 --enable-fpm 配置选项来激活 FPM 支持。 //https://www.php.net/manual/zh/install.fpm.install.php yum install libxml2-devel #原链接https://blog.csdn.net/have_a_cat/article/details/115208121 yum -y install sqlite-devel #原链接https://www.inbeijing.org/archives/2079 libcurl yum install -y libcurl-devel.x86_64 #原链接https://blog.csdn.net/weixin_43930641/article/details/106198442 oniguruma yum -y install oniguruma-devel #原链接https://blog.csdn.net/qq_17631419/article/details/106803807 提示没有软件包 yum install -y epel-release #原链接https://blog.csdn.net/shuiyuetianwy/article/details/86070213 第三步 编译安装 make make install #将文件复制一份 cp /data/soft/php-7.4.22/php.ini-development /data/server/php74/lib/php.ini cp /data/server/php74/etc/php-fpm.conf.default /data/server/php74/etc/php-fpm.conf cp /data/server/php74/etc/php-fpm.d/www.conf.default /data/server/php74/etc/php-fpm.d/www.conf 修改php.ini　cgi.fix_pathinfo=0 #原链接https://www.laruence.com/2010/05/20/1495.html 如果报错： failed to open configuration file \u0026#39;/data/server/php7/etc/php-fpm.conf\u0026#39;: No such file or directory (2) [22-Aug-2021 16:19:29] ERROR: failed to load configuration file \u0026#39;/data/server/php7/etc/php-fpm.conf\u0026#39; [22-Aug-2021 16:19:29] ERROR: FPM initialization failed 第四步 启动PHP cp php74 php7 #前提是在上面的/data/server目录下 ./sbin/php-fpm #启动 netstat -tnulp |grep php 查看 [bash: netstat: command not found] yum -y install net-tools 关闭 pkill php-fpm #修改nginx的配置文件 大致如下所示 vim nginx.conf server { listen 80; server_name localhost; #静态请求location #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.php index.html index.htm; } #动态请求处理的locatuion location ~* .*\\.(php|php7)?$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } 第五步 配置php的环境变量 export PATH=/data/server/php74/bin:$PATH 防火墙 防火墙的开启、关闭、禁用命令\n（1）设置开机启用防火墙：systemctl enable firewalld.service\n（2）设置开机禁用防火墙：systemctl disable firewalld.service\n（3）启动防火墙：systemctl start firewalld\n（4）关闭防火墙：systemctl stop firewalld\n（5）检查防火墙状态：systemctl status firewalld\n","permalink":"http://localhost:1313/2021/%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85lnmp/","summary":"lnmp 第一步创建一个总文件夹 mkdir /data/{soft,server} -p nginx 第一步 useradd www -s /sbin/nologin -M 第二步 从nginx官网下载它的包 wget http://nginx.org/download/nginx-1.6.3.tar.gz 第三步 解压nginx tar xzf nginx (tar xzf nginx-1.6.3.tar.gz) 第四步 将解压出来的nginx 放到指定目录 ./configure --prefix=/data/server/nginx] (cd nginx-1.6.3再执行该命令 linux环境下安装软件是./configure编译文件 --prefix 到指定目录) 在此期间如有报错 需要用yum安装的命令 [./configure: error: C compiler cc is not found] yum -y install gcc [he HTTP rewrite module requires the PCRE library.] yum -y install pcre-devel [./configure: error: the HTTP gzip module requires the zlib library.] yum install -y zlib-devel 第五步 编译安装 make \u0026amp;\u0026amp; make install 第六步 修改nginx的配置文件 vim /data/server/nginx/conf/nginx.","title":"源码安装lnmp"},{"content":"1.下载zookeeper 下载 # 进入下载目录并下载 cd /data wget https://mirrors.bfsu.edu.cn/apache/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz 解压 #解压 tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz #重命名 mv apache-zookeeper-3.7.0-bin zookeeper 修改配置文件 # 1.进入配置文件目录 cd zookeeper/conf #2.将zoo_sample.cfg这个文件复制为zoo.cfg (注意：文件名一定要是zoo.cfg) cp zoo_sample.cfg zoo.cfg #3.修改配置文件vi zoo.cfg 数据存放的文件 文件夹一定要存在 要不然会启动失败 mkdir /data/zookeeper/data vi zoo.cfg #修改文件里面的内容 dataDir = /data/zookeeper/data 启动 #进入文件夹 cd /data/zookeeper/bin #启动 ./zkServer.sh start # 停止 ./zkServer.sh stop # 重启 ./zkServer.sh restart # 查看状态 ./zkServer.sh status 设置zookeeper开机自动启动 cd /lib/systemd/system/ vi zookeeper.service #写入文件的内容 [Unit] Description=zookeeperservice After=network.target [Service] WorkingDirectory=/data/zookeeper #zookeeper路径 ExecStart=/data/zookeeper/bin/zkServer.sh start #zookeeper执行文件 User=root Group=root Restart=always RestartSec=10 [Install] WantedBy=multi-user.target #启动 systemctl start zookeeper.service #可能会用到文件修改权限组的问题 chown -R root:root zookeeper/ 2.kafka 下载 # 进入下载目录并下载 cd /data wget https://mirrors.bfsu.edu.cn/apache/kafka/2.8.1/kafka_2.12-2.8.1.tgz 解压 #解压 tar -zxvf kafka_2.12-2.8.1.tgz #重命名 mv kafka_2.12-2.8.1 kafka 修改配置文件 # 1.进入配置文件目录 cd kafka vi config/server.properties #3.修改配置文件vi zoo.cfg 数据存放的文件 文件夹一定要存在 要不然会启动失败 mkdir /data/kafka/logs #文件内容 log.dirs=/data/kafka/logs config下server.properties的解释 #//当前机器在集群中的唯一标识，和zookeeper的myid性质一样（broker.id和host.name每个节点都不相同） broker.id=0 #//当前kafka对外提供服务的端口默认是9092 listeners=PLAINTEXT://192.168.1.202:9092 #//这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。 host.name=hadoop1 #//这个是borker进行网络处理的线程数 num.network.threads=3 #//这个是borker进行I/O处理的线程数 num.io.threads=8 #//发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能 socket.send.buffer.bytes=102400 #//kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘 socket.receive.buffer.bytes=102400 #//这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小 socket.request.max.bytes=104857600 #//消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录， #//如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个 log.dirs=/home/hadoop/log/kafka-logs #//默认的分区数，一个topic默认1个分区数 num.partitions=1 #//每个数据目录用来日志恢复的线程数目 num.recovery.threads.per.data.dir=1 #//默认消息的最大持久化时间，168小时，7天 log.retention.hours=168 #//轮转时间，当需要删除指定小时之前的数据时，该设置项很重要 log.roll.hours=12 #//这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件 log.segment.bytes=1073741824 #//每隔300000毫秒去检查上面配置的log失效时间 log.retention.check.interval.ms=300000 #//是否启用log压缩，一般不用启用，启用的话可以提高性能 log.cleaner.enable=false #//设置zookeeper的连接端口 zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181 #//设置zookeeper的连接超时时间 zookeeper.connection.timeout.ms=6000 #metadata.broker.list=192.168.1.100:9092,192.168.1.101:9092,192.168.1.102:9092 #zookeeper.connect=192.168.1.100:2181,192.168.1.101:2181,192.168.1.102:2181 启动 #进入文件夹 cd /data/kafka #启动 bin/kafka-server-start.sh config/server.properties 设置kafka开机自动启动(在启动之前要确保zookeeper是在运行的) cd /lib/systemd/system/ vi kafka.service #写入文件的内容 [Unit] Description=kafkaservice After=network.target [Service] WorkingDirectory=/data/kafka ExecStart=/data/kafka/bin/kafka-server-start.sh config/server.properties ExecStop=/data/kafka/bin/kafka-server-stop.sh User=root Group=root Restart=always RestartSec=10 [Install] WantedBy=multi-user.target #启动 systemctl start kafka.service #可能会用到文件修改权限组的问题 chown -R root:root kafka/ 启动 #设置自启动 systemctl enable kafka.service #立即启动服务 systemctl start kafka.service #查看启动状态 systemctl status kafka.service ","permalink":"http://localhost:1313/2021/linux%E5%AE%89%E8%A3%85zookeeper%E4%B8%8Ekafka/","summary":"1.下载zookeeper 下载 # 进入下载目录并下载 cd /data wget https://mirrors.bfsu.edu.cn/apache/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz 解压 #解压 tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz #重命名 mv apache-zookeeper-3.7.0-bin zookeeper 修改配置文件 # 1.进入配置文件目录 cd zookeeper/conf #2.将zoo_sample.cfg这个文件复制为zoo.cfg (注意：文件名一定要是zoo.cfg) cp zoo_sample.cfg zoo.cfg #3.修改配置文件vi zoo.cfg 数据存放的文件 文件夹一定要存在 要不然会启动失败 mkdir /data/zookeeper/data vi zoo.cfg #修改文件里面的内容 dataDir = /data/zookeeper/data 启动 #进入文件夹 cd /data/zookeeper/bin #启动 ./zkServer.sh start # 停止 ./zkServer.sh stop # 重启 ./zkServer.sh restart # 查看状态 ./zkServer.sh status 设置zookeeper开机自动启动 cd /lib/systemd/system/ vi zookeeper.service #写入文件的内容 [Unit] Description=zookeeperservice After=network.target [Service] WorkingDirectory=/data/zookeeper #zookeeper路径 ExecStart=/data/zookeeper/bin/zkServer.","title":"linux安装zookeeper与kafka"},{"content":"这里是我的友链页面，展示了我的朋友链接。 ","permalink":"http://localhost:1313/friends/","summary":"friends","title":"友链"}]